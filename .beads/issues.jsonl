{"id":"locdoc-016","title":"Research headless browser reliability improvements","description":"## Background\nDuring locdoc-87y (preview reliability fix), we observed that headless Chrome pages can get 'stuck' and timeout, but complete instantly on retry. Web research confirms this is a well-documented issue.\n\n## Key Findings\n\n### The Problem\n- Pages timeout at 30s in headless mode but complete in ~5s with headed mode\n- Concurrent tabs cause memory buildup leading to slowdowns\n- Tab state corruption can cause hangs that fresh requests avoid\n\n### Evidence from TanStack docs test\n| Page | First attempt | Retry |\n|------|---------------|-------|\n| examples/chat | 32s timeout | 1.5s success |\n| examples/nextjs | 31s timeout | 1.6s success |\n| examples/suspense | 30s timeout | 1.8s success |\n\nRetries completed 20x faster - suggests stuck state, not slow page.\n\n### Community Solutions\n1. **Retry logic** (implemented in locdoc-87y)\n2. **Lower concurrency** (implemented - default now 3)\n3. **Chrome flags** for memory/stability:\n   - `--disable-dev-shm-usage`\n   - `--disable-background-networking`\n   - `--memory-pressure-off`\n   - `--disable-background-timer-throttling`\n4. **Tab cleanup** - navigate to about:blank after use\n5. **Memory limits** - cgroups/Docker memory constraints\n\n## Research Questions\n- Which Chrome flags provide best reliability vs performance tradeoff?\n- Should we periodically restart browser instance after N pages?\n- Is there value in per-request browser instances vs shared browser?\n- What's the optimal concurrency for different machine specs?\n\n## References\n- https://github.com/puppeteer/puppeteer/issues/1922\n- https://github.com/puppeteer/puppeteer/issues/3314\n- https://webscraping.ai/faq/headless-chromium/what-are-the-best-practices-for-managing-memory-usage-in-headless-chromium\n\n## Next Steps\nUse Claude deep research to analyze headless browser reliability patterns and make actionable recommendations.","notes":"RESEARCH COMPLETE\n\n## Root Cause Confirmed\nPages timeout because Chrome aggressively throttles 'background' tabs. With concurrent tabs, the tab waiting for load events gets deprioritized, and lifecycle events fire with massive delays or not at all.\n\n## Ranked Recommendations\n\n### Priority 1: Chrome Flags (5 min, Very High impact)\nAdd to launcher:\n- --disable-background-timer-throttling (CRITICAL)\n- --disable-backgrounding-occluded-windows (CRITICAL)\n- --disable-renderer-backgrounding\n- --disable-dev-shm-usage\n- --disable-hang-monitor\n\n### Priority 2: WaitStable() (5 min, High impact)\nReplace WaitLoad() with WaitStable() - it combines multiple checks in parallel so if one event fails to fire, others can still complete.\n\n### Priority 3: Incognito Contexts (15 min, High impact)\nUse browser.MustIncognito() per request instead of raw page creation. Provides isolation with negligible overhead (ms vs 2-3s for new browser).\n\n### Priority 4: Fix Page Close (15 min, Medium impact)\nUse fresh context for cleanup - the cancelled timeout context will also fail page.Close().\n\n### Priority 5: Browser Recycling (30 min, Medium impact)\nRecycle browser every 50-100 pages. Memory grows ~0.5MB/s under load.\n\n## Implementation Tasks Created\n- locdoc-3ks: Add Chrome stability flags to rod launcher [P2]\n- locdoc-y27: Switch to WaitStable and incognito contexts [P2] (depends on locdoc-3ks)\n- locdoc-her: Add browser recycling to prevent memory accumulation [P3] (depends on locdoc-y27)\n\n## Reference\nFull research documented in docs/go-rod-reliability.md","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-20T10:24:13.044784-08:00","updated_at":"2025-12-20T10:56:47.321989-08:00","closed_at":"2025-12-20T10:56:16.367518-08:00"}
{"id":"locdoc-06y","title":"Remove unused RAG interfaces (ChunkService, SearchService)","description":"## Problem\n\nWe decided not to implement RAG initially, so the ChunkService and SearchService interfaces are unused and should be removed to reduce codebase complexity.\n\n## Entrypoints\n\n- `chunk.go` - contains all the types to remove\n\n## Work Required\n\nRemove the following from `chunk.go`:\n- `Chunk` struct\n- `ChunkMetadata` struct\n- `Chunk.Validate()` method\n- `ChunkService` interface\n- `ChunkFilter` struct\n- `SearchService` interface\n- `SearchOptions` struct\n- `SearchResult` struct\n\nIf the file becomes empty after removal, delete it entirely.\n\nAlso check `mock/` for any related mocks and remove them.\n\n## Validation\n\n- [ ] All RAG-related types removed from chunk.go\n- [ ] No orphaned mocks remain\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-09T20:16:34.374725-08:00","updated_at":"2025-12-09T20:23:38.153302-08:00","closed_at":"2025-12-09T20:23:38.153305-08:00"}
{"id":"locdoc-0fo","title":"Verify golangci-lint configuration alignment","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-07T16:40:32.010199-08:00","updated_at":"2025-12-07T17:52:59.10034-08:00","closed_at":"2025-12-07T17:52:59.100343-08:00","dependencies":[{"issue_id":"locdoc-0fo","depends_on_id":"locdoc-hw3","type":"blocks","created_at":"2025-12-07T16:40:55.276865-08:00","created_by":"daemon"}]}
{"id":"locdoc-0kg","title":"Filter sitemap URLs by source URL path prefix","description":"## Problem\n\nWhen adding a project with a path-specific URL like `https://htmx.org/docs/`, the sitemap discovery returns URLs for the entire site. This results in crawling unrelated pages (e.g., `/examples/`, `/essays/`) when the user only wanted `/docs/`.\n\n## Entrypoints\n\n- `http/sitemap.go` (SitemapService.DiscoverURLs)\n- `locdoc.go` (URLFilter struct)\n\n## Implementation\n\nWhen the source URL has a non-root path (e.g., `/docs/`), automatically filter sitemap results to only include URLs that start with that path prefix.\n\nExamples:\n- Source: `https://htmx.org/docs/` → only crawl URLs starting with `/docs/`\n- Source: `https://htmx.org/` → crawl all URLs (no filtering)\n- Source: `https://example.com/api/v2/` → only crawl URLs starting with `/api/v2/`\n\n## Validation\n\n- [ ] `locdoc add foo https://htmx.org/docs/` followed by `locdoc crawl foo` only fetches `/docs/*` pages\n- [ ] `locdoc add bar https://htmx.org/` crawls all pages (no filtering)\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-09T21:43:17.869922-08:00","updated_at":"2025-12-10T08:46:37.440774-08:00","closed_at":"2025-12-10T08:46:37.440777-08:00"}
{"id":"locdoc-0ox","title":"Add probe logic to DiscoverURLs for preview mode","description":"## Problem\nPreview mode (DiscoverURLs) needs same adaptive rendering as full crawl.\n\n## Entrypoints\n- crawl/discover.go\n\n## Implementation\nSame probe logic as CrawlProject:\n1. HTTP fetch first URL\n2. Detect framework, check RequiresJS\n3. If unknown, compare content\n4. Use chosen fetcher for discovery\n\nMay need to refactor probe logic into shared function.\n\n## Validation\n- [ ] Preview mode uses HTTP for static sites\n- [ ] Preview mode uses Rod for JS sites\n- [ ] Consistent behavior with full crawl\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T14:58:55.821886-08:00","updated_at":"2025-12-20T17:47:19.359816-08:00","closed_at":"2025-12-20T17:47:19.359819-08:00","dependencies":[{"issue_id":"locdoc-0ox","depends_on_id":"locdoc-1u6","type":"blocks","created_at":"2025-12-20T14:59:11.974743-08:00","created_by":"daemon"}]}
{"id":"locdoc-1h6","title":"Create domain types skeleton","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-07T16:40:32.199767-08:00","updated_at":"2025-12-07T16:56:36.84987-08:00","closed_at":"2025-12-07T16:56:36.849873-08:00","dependencies":[{"issue_id":"locdoc-1h6","depends_on_id":"locdoc-knx","type":"blocks","created_at":"2025-12-07T16:40:59.902041-08:00","created_by":"daemon"}]}
{"id":"locdoc-1kz","title":"Stream preview links as they are discovered","description":"## Problem\nPreview mode waits until all pages are crawled before displaying any links. For large sites (like TanStack with 279+ URLs), this means a long wait with no feedback.\n\n## Proposed Solution\nDisplay links as they are discovered during the crawl:\n- Print each unique URL as it's found\n- Deduplicate on the fly (don't print duplicates)\n- Final summary could show total count\n\n## Considerations\n- May need to rethink the crawl architecture (currently returns all at once)\n- Could use a channel-based approach for streaming results\n- Consider sorting implications (currently results may be ordered by priority)\n\n## Entrypoints\n- cmd/locdoc/main.go (preview output logic)\n- crawl/ package (result streaming)\n\n## Validation\n- `locdoc add --preview \u003cname\u003e \u003curl\u003e` shows URLs as they're found\n- Large sites show progressive output instead of long wait\n- make validate passes","notes":"COMPLETED: Streaming preview URLs implementation\n- Added WithOnURL callback option to crawl.DiscoverURLs\n- Updated CLI add.go to use streaming callback for recursive discovery\n- Sitemap discovery still returns all at once (by design - sitemap XML is fetched atomically)\n- Recursive crawl now streams URLs as they're discovered\n\nVALIDATION: make validate passes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-19T23:52:36.551437-08:00","updated_at":"2025-12-20T14:25:34.341758-08:00","closed_at":"2025-12-20T14:25:34.341762-08:00"}
{"id":"locdoc-1m8","title":"Add RequiresJS method to goquery.Detector","description":"## Problem\nNeed to determine if a detected framework requires JavaScript rendering.\n\n## Entrypoints\n- goquery/detector.go\n\n## Implementation\nAdd method to Detector:\n```go\nfunc (d *Detector) RequiresJS(framework locdoc.Framework) (requires bool, known bool)\n```\n\nFramework mapping:\n- GitBook → (true, true)\n- Sphinx, MkDocs, Docusaurus, VitePress, VuePress, Nextra → (false, true)\n- Unknown → (false, false)\n\n## Validation\n- [x] Unit tests for all known frameworks\n- [x] Unknown framework returns known=false\n- [x] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T14:51:07.015288-08:00","updated_at":"2025-12-20T15:11:24.642097-08:00","closed_at":"2025-12-20T15:11:24.6421-08:00"}
{"id":"locdoc-1tj","title":"Add content comparison function","description":"## Problem\nFor unknown frameworks, need to compare HTTP vs Rod fetched content to determine if JS rendering adds significant content.\n\n## Entrypoints\n- crawl/compare.go (new file)\n\n## Implementation\n```go\nfunc contentDiffers(httpHTML, rodHTML string, extractor locdoc.Extractor) bool\n```\n\n- Extract markdown from both using Extractor\n- Compare lengths: if Rod content \u003e50% longer, return true\n- Handle extraction errors by returning true (assume JS needed)\n\n## Validation\n- [ ] Returns true when Rod content is \u003e50% longer\n- [ ] Returns false when content is similar\n- [ ] Handles extraction errors gracefully\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T14:58:40.683831-08:00","updated_at":"2025-12-20T16:53:56.277538-08:00","closed_at":"2025-12-20T16:53:56.277541-08:00"}
{"id":"locdoc-1u6","title":"Add probe logic to Crawler.CrawlProject","description":"## Problem\nCrawler needs to probe first URL and choose fetcher before crawling.\n\n## Entrypoints\n- crawl/crawl.go\n\n## Implementation\nAdd fields to Crawler:\n```go\nHTTPFetcher locdoc.Fetcher\nRodFetcher  locdoc.Fetcher\n```\n\nAt start of CrawlProject:\n1. HTTP fetch first URL\n2. Detect framework via Detector\n3. Call RequiresJS to check rendering requirement\n4. If known → choose fetcher based on requirement\n5. If unknown → Rod fetch, compare content, choose fetcher\n6. If HTTP failed → fall back to Rod\n7. Use chosen fetcher for rest of crawl\n\n## Validation\n- [ ] Known HTTP-only framework uses HTTP fetcher\n- [ ] Known JS framework uses Rod fetcher\n- [ ] Unknown framework with different content uses Rod\n- [ ] HTTP probe failure falls back to Rod\n- [ ] make validate passes","notes":"COMPLETED: All probe logic implemented and tested\n- Added Prober interface with Detect and RequiresJS methods\n- Added HTTPFetcher and RodFetcher fields to Crawler\n- Implemented probeFetcher method with logic:\n  1. HTTP probe first URL\n  2. Detect framework\n  3. Known framework → use HTTP or Rod based on RequiresJS\n  4. Unknown framework → compare content, choose based on differences\n  5. HTTP failure → fall back to Rod\n- All 4 validation tests passing:\n  - Known HTTP-only framework uses HTTP fetcher\n  - Known JS framework uses Rod fetcher  \n  - Unknown framework with different content uses Rod\n  - HTTP probe failure falls back to Rod\n- make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T14:58:49.907096-08:00","updated_at":"2025-12-20T17:29:59.877352-08:00","closed_at":"2025-12-20T17:29:59.877355-08:00","dependencies":[{"issue_id":"locdoc-1u6","depends_on_id":"locdoc-1m8","type":"blocks","created_at":"2025-12-20T14:59:11.763153-08:00","created_by":"daemon"},{"issue_id":"locdoc-1u6","depends_on_id":"locdoc-bky","type":"blocks","created_at":"2025-12-20T14:59:11.821104-08:00","created_by":"daemon"},{"issue_id":"locdoc-1u6","depends_on_id":"locdoc-1tj","type":"blocks","created_at":"2025-12-20T14:59:11.877131-08:00","created_by":"daemon"}]}
{"id":"locdoc-1z8","title":"Create GitHub Actions CI workflow","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-07T16:40:32.10089-08:00","updated_at":"2025-12-07T18:08:02.712841-08:00","closed_at":"2025-12-07T18:08:02.712844-08:00","dependencies":[{"issue_id":"locdoc-1z8","depends_on_id":"locdoc-knx","type":"blocks","created_at":"2025-12-07T16:40:59.837864-08:00","created_by":"daemon"},{"issue_id":"locdoc-1z8","depends_on_id":"locdoc-0fo","type":"blocks","created_at":"2025-12-07T16:40:59.962233-08:00","created_by":"daemon"}]}
{"id":"locdoc-296","title":"Add Position field to Document domain type","description":"## Problem\n\nDocuments need a Position field to preserve sitemap order for coherent LLM context.\n\n## Entrypoints\n\n- `/Users/filip/code/go/locdoc/document.go`\n\n## Implementation\n\nAdd `Position int` to `Document` struct (after ContentHash, before FetchedAt):\n```go\nPosition int `json:\"position\"`\n```\n\nAdd `Position *int` to `DocumentUpdate`:\n```go\nPosition *int `json:\"position\"`\n```\n\nAdd `SortBy string` to `DocumentFilter`:\n```go\nSortBy string `json:\"sortBy\"` // \"position\" or \"fetched_at\"\n```\n\n## Validation\n\n- [ ] Types compile\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-09T17:57:02.201376-08:00","updated_at":"2025-12-09T18:55:31.409109-08:00","closed_at":"2025-12-09T18:55:31.409112-08:00"}
{"id":"locdoc-2ni","title":"Add --timeout flag for fetch timeout","description":"## Problem\nDefault 10s timeout may be too short for some heavy JS pages.\n\n## Solution\nAdded --timeout flag (-t) to customize fetch timeout, defaulting to 10s.\n\n## Usage\n`locdoc add --preview --timeout 60s ...` allows longer waits\n\n## Validation\n- Flag parsed correctly with default 10s\n- Custom timeout passed to rod.NewFetcher\n- make validate passes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-20T09:55:33.691616-08:00","updated_at":"2025-12-20T14:07:40.194527-08:00","closed_at":"2025-12-20T14:07:40.194531-08:00"}
{"id":"locdoc-2pt","title":"Add --concurrency flag to locdoc add command","description":"## Problem\nConcurrency is hardcoded at 20, not tunable for different machines or sites.\n\n## Entrypoints\n- `cmd/locdoc/main.go` - `ParseAddArgs()` and `crawlProject()`\n\n## Validation\n- [ ] `--concurrency N` / `-c N` flag added to `locdoc add`\n- [ ] Default is 10 (documented in help text)\n- [ ] Flag value used instead of hardcoded 20\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T15:41:12.141973-08:00","updated_at":"2025-12-10T17:47:09.136086-08:00","closed_at":"2025-12-10T17:47:09.136088-08:00"}
{"id":"locdoc-2yj","title":"Support crawling sites without sitemap.xml","description":"Implement recursive link crawling with navigation-aware extraction as fallback when sitemap discovery fails.\n\n## Design Decisions\n- Trigger: Fallback only (sitemap first, recursive if 0 URLs)\n- Persistence: None (in-memory frontier)\n- Scope: Path prefix limiting\n- Frameworks: Full support + generic fallback\n\nSee plan: ~/.claude/plans/recursive-spinning-milner.md","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-18T16:07:18.827188-08:00","updated_at":"2025-12-19T23:32:52.218162-08:00","closed_at":"2025-12-19T23:32:52.218163-08:00"}
{"id":"locdoc-2yj.1","title":"Add bounded concurrency to recursive crawling","description":"Replace sequential URL processing in recursiveCrawl with a coordinator pattern for concurrent fetching.\n\n## Design\nSee: docs/plans/2025-12-18-concurrent-recursive-crawl.md\n\n## Entrypoints\n- crawl/crawl.go: Refactor recursiveCrawl() to coordinator pattern\n- crawl/crawl.go: Extend crawlResult with discovered field\n- crawl/crawl_test.go: Add concurrency tests\n\n## Approach\n1. Extend crawlResult struct with discovered []DiscoveredLink\n2. Create worker pool (N = Concurrency) reading from work channel\n3. Implement coordinator loop: pop from frontier, dispatch to workers, receive results, push discovered URLs\n4. Handle termination: frontier empty AND pending == 0\n5. Handle context cancellation with graceful drain\n\n## Validation\n- [ ] Concurrent workers process URLs in parallel\n- [ ] Rate limiter still enforces per-domain limits\n- [ ] Termination correct when frontier drains\n- [ ] Context cancellation returns partial results\n- [ ] Max URL limit still enforced\n- [ ] Existing recursive crawl tests still pass\n- [ ] make validate passes","notes":"COMPLETED: Concurrent recursive crawling implementation\n- Extended crawlResult with discovered field for link passing\n- Implemented coordinator pattern with worker pool\n- Added concurrency tests verifying parallel processing\n- Rate limiter tests, max URL limit tests pass\n- All existing tests pass\n- make validate passes\n\nKEY_DECISIONS:\n- Workers handle fetch/extract/convert, coordinator handles scope filtering\n- Unbuffered result channel for backpressure\n- 5 second drain timeout on context cancellation\n- Position assignment happens in coordinator (sequential)","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-18T20:13:46.104768-08:00","updated_at":"2025-12-19T21:22:43.352607-08:00","closed_at":"2025-12-19T21:22:43.35261-08:00","dependencies":[{"issue_id":"locdoc-2yj.1","depends_on_id":"locdoc-2yj","type":"parent-child","created_at":"2025-12-18T20:13:46.105649-08:00","created_by":"daemon"}]}
{"id":"locdoc-2zl","title":"Integrate crawl logic into add command","description":"Move crawl functionality into the add command so add creates project AND crawls in one step.\n\n## Behavior\n- add \u003cname\u003e \u003curl\u003e creates project then immediately crawls\n- Remove hash-based diffing logic (always fresh crawl)\n- Errors if project already exists (without --force)\n\n## Entrypoints\n- cmd/locdoc/main.go (CmdAdd, crawlProject)\n\n## Validation\n- [ ] locdoc add \u003cname\u003e \u003curl\u003e creates and crawls\n- [ ] Error if project exists\n- [ ] No diffing logic (simpler code)\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T11:06:23.718095-08:00","updated_at":"2025-12-10T12:58:34.149813-08:00","closed_at":"2025-12-10T12:58:34.149817-08:00","dependencies":[{"issue_id":"locdoc-2zl","depends_on_id":"locdoc-612","type":"blocks","created_at":"2025-12-10T11:06:44.505494-08:00","created_by":"daemon"}]}
{"id":"locdoc-3if","title":"Restore live progress display for add command","description":"## Problem\nThe add command lost its live progress display during the CLI refactor to Kong. Currently, the progress callback in `cmd/locdoc/add.go` only reports:\n- Initial URL count (ProgressStarted)\n- Failed URLs (ProgressFailed)\n\nIt ignores ProgressCompleted and ProgressFinished events, so users don't see live progress during crawling.\n\n## Entrypoints\n- `cmd/locdoc/add.go:81-90` - progress callback that needs enhancement\n- `crawl/crawl.go:39-59` - ProgressEvent types (infrastructure is ready)\n- Git history: commits 97af7c9 and a3217d6 had live progress with `\\r` updates\n\n## Validation\n- [ ] Progress line updates in-place during crawl showing `[N/M]` completion\n- [ ] Failed URLs still print on separate lines\n- [ ] Final summary shows totals\n- [ ] `make validate` passes","notes":"COMPLETED: Implemented live progress display\n- Progress callback now handles ProgressCompleted events\n- Shows [N/M] format with carriage return for in-place updates\n- Failed URLs print on separate lines (persists in scroll history)\n- Progress line cleared on ProgressFinished\n- Tests added and passing\n- make validate passes\n\nREADY FOR: /finish-task","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T11:16:34.17704-08:00","updated_at":"2025-12-12T11:24:55.250771-08:00","closed_at":"2025-12-12T11:24:55.250774-08:00"}
{"id":"locdoc-3ks","title":"Add Chrome stability flags to rod launcher","description":"## Problem\nChrome aggressively throttles 'background' tabs during concurrent operations, causing lifecycle events to fire with massive delays or not at all.\n\n## Solution\nAdd these flags to launcher in rod/fetcher.go:\n\n```go\nlauncher.New().\n    Set(\"disable-background-timer-throttling\").      // CRITICAL: prevents timer delays\n    Set(\"disable-backgrounding-occluded-windows\").   // CRITICAL: prevents deprioritizing tabs\n    Set(\"disable-renderer-backgrounding\").           // Keeps all renderers at full priority\n    Set(\"disable-dev-shm-usage\").                    // Essential for Docker\n    Set(\"disable-hang-monitor\").                     // Prevents killing heavy pages\n    Leakless(true).                                  // Auto-kill on exit\n    Headless(true)\n```\n\n## Entrypoints\n- rod/fetcher.go:50\n\n## Validation\n- [ ] Flags added to launcher\n- [ ] make validate passes\n\n## Research\nSee docs/go-rod-reliability.md for full context.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T10:54:46.642709-08:00","updated_at":"2025-12-20T11:07:05.388163-08:00","closed_at":"2025-12-20T11:07:05.388166-08:00"}
{"id":"locdoc-4jw","title":"Implement documentation framework detector","description":"Create FrameworkDetector that identifies doc frameworks (Docusaurus, MkDocs, Sphinx, etc.) from HTML.\n\n## Entrypoints\n- Create goquery/detector.go - Detector struct checking framework-specific markers\n- Create goquery/detector_test.go with HTML samples from each framework\n\n## Validation\n- Correctly identifies all 6 supported frameworks\n- Returns FrameworkUnknown for unrecognized HTML\n- make validate passes","notes":"COMPLETED: FrameworkDetector with research-validated markers\n\nSupports all 7 frameworks defined in linkselector.go:\n- Docusaurus, MkDocs, Sphinx, VitePress, VuePress, GitBook, Nextra\n\nDetection methods (in priority order):\n1. Meta generator tag (most reliable for Sphinx, GitBook)\n2. Framework-specific CSS classes and data attributes\n\nTests: 21 test cases covering all frameworks + priority order + edge cases\nDetector is stateless and safe for concurrent use.\nmake validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T16:07:52.882025-08:00","updated_at":"2025-12-18T18:11:24.559617-08:00","closed_at":"2025-12-18T18:11:24.55962-08:00","dependencies":[{"issue_id":"locdoc-4jw","depends_on_id":"locdoc-2yj","type":"parent-child","created_at":"2025-12-18T16:16:39.671053-08:00","created_by":"daemon"},{"issue_id":"locdoc-4jw","depends_on_id":"locdoc-phn","type":"blocks","created_at":"2025-12-18T16:18:20.027333-08:00","created_by":"daemon"}]}
{"id":"locdoc-4nk","title":"Implement gemini/ package","description":"## Problem\n\nNeed to implement the `Asker` interface using Google Gemini API.\n\n## Entrypoints\n\n- Create `gemini/gemini.go`\n\n## Implementation\n\n- Use `google.golang.org/genai` client\n- Read API key from `GEMINI_API_KEY` env var\n- Construct prompt from documents + question\n- Use `gemini-2.0-flash` model\n- Return plain text answer\n\n## Prompt Template\n\n```\nYou are a helpful assistant answering questions about software library documentation.\n\n\u003cdocumentation\u003e\n## Document: {title or source URL}\n{content}\n...\n\u003c/documentation\u003e\n\nQuestion: {question}\n\nAnswer based only on the documentation provided. If the answer is not in the documentation, say so.\n```\n\n## Validation\n\n- [ ] Package compiles\n- [ ] Mock added to `mock/` package\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-09T16:28:52.543185-08:00","updated_at":"2025-12-09T20:55:10.347225-08:00","closed_at":"2025-12-09T20:55:10.347229-08:00","dependencies":[{"issue_id":"locdoc-4nk","depends_on_id":"locdoc-amg","type":"blocks","created_at":"2025-12-09T16:29:04.564441-08:00","created_by":"daemon"}]}
{"id":"locdoc-4qp","title":"Update SQLite schema and implementation for Position","description":"## Problem\n\nSQLite needs position column and queries need to handle it.\n\n## Entrypoints\n\n- `/Users/filip/code/go/locdoc/sqlite/sqlite.go` - schema\n- `/Users/filip/code/go/locdoc/sqlite/document.go` - queries\n\n## Implementation\n\n**Schema** - add column to documents table:\n```sql\nposition INTEGER NOT NULL DEFAULT 0\n```\n\n**CreateDocument** - include position in INSERT\n\n**FindDocumentByID** - include position in SELECT and Scan\n\n**FindDocuments** - include position in SELECT/Scan, respect SortBy:\n```go\nswitch filter.SortBy {\ncase \"position\":\n    query.WriteString(\" ORDER BY position ASC\")\ndefault:\n    query.WriteString(\" ORDER BY fetched_at DESC\")\n}\n```\n\n**UpdateDocument** - handle Position in update\n\n## Validation\n\n- [ ] Schema includes position column\n- [ ] All queries handle position\n- [ ] SortBy respected in FindDocuments\n- [ ] Existing tests pass\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-09T17:57:08.935564-08:00","updated_at":"2025-12-09T19:01:18.324679-08:00","closed_at":"2025-12-09T19:01:18.324681-08:00","dependencies":[{"issue_id":"locdoc-4qp","depends_on_id":"locdoc-296","type":"blocks","created_at":"2025-12-09T17:57:42.54959-08:00","created_by":"daemon"}]}
{"id":"locdoc-4se","title":"Update ask command design document for ordering","description":"## Problem\n\nThe ask command design document should mention document ordering.\n\n## Entrypoints\n\n- `/Users/filip/code/go/locdoc/docs/plans/2025-12-09-ask-command-design.md`\n\n## Changes\n\nUpdate the design doc to note:\n- Documents are fetched in sitemap position order\n- This provides coherent LLM context (getting started before advanced topics)\n- Use `SortBy: \"position\"` when fetching documents for ask command\n\n## Validation\n\n- [ ] Design doc updated\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-09T17:57:32.523577-08:00","updated_at":"2025-12-09T18:52:24.414431-08:00","closed_at":"2025-12-09T18:52:24.414434-08:00"}
{"id":"locdoc-56t","title":"Add locdoc delete command","description":"## Problem\n\nNeed CLI command to delete projects.\n\n## Entrypoints\n\n- `cmd/locdoc/main.go` - Add delete command dispatch\n- Interface and sqlite implementation already exist (`ProjectService.DeleteProject`)\n\n## Requirements\n\n- `locdoc delete \u003cname\u003e` - delete project by name\n- Confirm before deleting (or add --force flag)\n- Cascade deletes associated documents (handled by sqlite foreign keys)\n\n## Validation\n\n- [ ] Unit tests for cmdDelete\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-09T14:25:01.662287-08:00","updated_at":"2025-12-09T15:05:18.77442-08:00","closed_at":"2025-12-09T15:05:18.774424-08:00"}
{"id":"locdoc-57u","title":"Add explicit fetch timeout to page navigation","description":"## Problem\nPage fetching relies on Rod's default timeout, giving no explicit control over how long to wait for slow pages.\n\n## Entrypoints\n- `rod/fetcher.go` - Fetcher implementation\n\n## Validation\n- [ ] Page navigation uses explicit 30-second timeout\n- [ ] Timeout errors surface clearly in fetch failures\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T15:41:02.475682-08:00","updated_at":"2025-12-10T16:46:51.023551-08:00","closed_at":"2025-12-10T16:46:51.023554-08:00"}
{"id":"locdoc-5kd","title":"Integrate recursive crawling into Crawler","description":"Modify crawl.Crawler to use recursive crawling as fallback when sitemap returns 0 URLs. Includes per-domain rate limiting.\n\n## Entrypoints\n- Create crawl/ratelimit.go - DomainLimiter using x/time/rate\n- Create crawl/ratelimit_test.go\n- Modify crawl/crawl.go:\n  - Add LinkSelectors and RateLimiter fields\n  - Add recursiveCrawl() method\n  - Modify CrawlProject() to fall back to recursive crawl\n- Update crawl/crawl_test.go\n\n## Validation\n- Rate limiter blocks when limit exceeded\n- Different domains have independent limits\n- Sitemap discovery still works (existing tests pass)\n- Falls back to recursive when sitemap returns 0 URLs\n- Respects path prefix scope\n- make validate passes","notes":"COMPLETED: All implementation\n- DomainLimiter with per-domain rate limiting using x/time/rate\n- Crawler struct extended with LinkSelectors and RateLimiter fields  \n- recursiveCrawl method implementing BFS with frontier and dedup\n- CrawlProject falls back to recursive crawl when sitemap returns 0 URLs\n- Path prefix scoping to limit crawl to source URL path\n- Tests for rate limiting, path scoping, fallback behavior\n- make validate passes\n\nREADY FOR: /finish-task","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T16:08:11.001765-08:00","updated_at":"2025-12-18T19:37:46.447802-08:00","closed_at":"2025-12-18T19:37:46.447805-08:00","dependencies":[{"issue_id":"locdoc-5kd","depends_on_id":"locdoc-2yj","type":"parent-child","created_at":"2025-12-18T16:16:39.980475-08:00","created_by":"daemon"},{"issue_id":"locdoc-5kd","depends_on_id":"locdoc-ys8","type":"blocks","created_at":"2025-12-18T16:18:25.130699-08:00","created_by":"daemon"},{"issue_id":"locdoc-5kd","depends_on_id":"locdoc-lf5","type":"blocks","created_at":"2025-12-18T16:18:25.188686-08:00","created_by":"daemon"},{"issue_id":"locdoc-5kd","depends_on_id":"locdoc-l3o","type":"blocks","created_at":"2025-12-18T16:18:25.250847-08:00","created_by":"daemon"}]}
{"id":"locdoc-5lp","title":"Implement framework-specific link selectors","description":"Create selectors for Docusaurus, MkDocs, Sphinx, VuePress/VitePress, GitBook, and Nextra.\n\n## Entrypoints\n- Create goquery/selector_docusaurus.go\n- Create goquery/selector_mkdocs.go\n- Create goquery/selector_sphinx.go\n- Create goquery/selector_vuepress.go (covers VitePress)\n- Create goquery/selector_gitbook.go\n- Create goquery/selector_nextra.go\n- Tests for each\n\n## Validation\n- Each selector extracts correct navigation links from framework HTML\n- make validate passes","notes":"COMPLETED: All 6 framework-specific link selectors implemented\n\nCreated:\n- goquery/selector_docusaurus.go (+ test)\n- goquery/selector_mkdocs.go (+ test)  \n- goquery/selector_sphinx.go (+ test)\n- goquery/selector_vuepress.go (+ test) - covers VitePress too\n- goquery/selector_gitbook.go (+ test)\n- goquery/selector_nextra.go (+ test)\n\nEach selector:\n- Implements locdoc.LinkSelector interface\n- Extracts links from framework-specific HTML elements\n- Assigns correct priorities (TOC \u003e Navigation \u003e Content \u003e Footer)\n- Filters external links\n- Deduplicates keeping highest priority\n\nAll tests pass, make validate passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T16:07:53.09838-08:00","updated_at":"2025-12-18T18:45:35.555877-08:00","closed_at":"2025-12-18T18:45:35.55588-08:00","dependencies":[{"issue_id":"locdoc-5lp","depends_on_id":"locdoc-2yj","type":"parent-child","created_at":"2025-12-18T16:16:39.736736-08:00","created_by":"daemon"},{"issue_id":"locdoc-5lp","depends_on_id":"locdoc-nwx","type":"blocks","created_at":"2025-12-18T16:18:22.686022-08:00","created_by":"daemon"},{"issue_id":"locdoc-5lp","depends_on_id":"locdoc-4jw","type":"blocks","created_at":"2025-12-18T16:18:22.746274-08:00","created_by":"daemon"}]}
{"id":"locdoc-5ma","title":"Add bounded concurrency to crawl command","description":"## Problem\n\nCrawling is slow because URLs are fetched sequentially. With 179 URLs, this takes several minutes.\n\n## Entrypoints\n\n- cmd/locdoc/main.go (crawlProject function)\n\n## Implementation\n\nAdd bounded concurrency (e.g., 5-10 workers) while preserving document position ordering:\n1. Create a worker pool with semaphore\n2. Process URLs concurrently\n3. Collect results with their original position index\n4. Store documents with correct position values\n\nConsider using errgroup with SetLimit() for bounded concurrency.\n\n## Validation\n\n- [ ] Crawl is noticeably faster\n- [ ] Document positions are preserved correctly\n- [ ] Error handling still works (skip failed URLs)\n- [ ] make validate passes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-09T20:57:11.594455-08:00","updated_at":"2025-12-09T21:30:15.160985-08:00","closed_at":"2025-12-09T21:30:15.160989-08:00"}
{"id":"locdoc-612","title":"Add --preview flag to add command","description":"Add --preview flag that shows sitemap URLs without creating project or crawling.\n\n## Behavior\n- Discovers URLs from sitemap\n- Prints URL list to stdout\n- Does NOT create project record\n- Does NOT crawl\n\n## Entrypoints\n- cmd/locdoc/main.go (CmdAdd)\n\n## Validation\n- [ ] locdoc add foo http://example.com --preview shows URLs\n- [ ] No project created with --preview\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T11:06:11.515317-08:00","updated_at":"2025-12-10T12:15:44.829601-08:00","closed_at":"2025-12-10T12:15:44.829605-08:00"}
{"id":"locdoc-63p","title":"Implement AddCmd with Kong","description":"Create cmd/locdoc/add.go with AddCmd struct and Run method. Wire to use crawl.Crawler for crawling. Handle --preview mode (sitemap discovery only). Handle --force mode (delete existing). Progress callback writes to stdout/stderr.","acceptance_criteria":"- [ ] cmd/locdoc/add.go exists\n- [ ] AddCmd struct with all flags (Name, URL, Preview, Force, Filter, Concurrency)\n- [ ] Run method delegates to Crawler.CrawlProject\n- [ ] Preview mode works (shows URLs without crawling)\n- [ ] Force mode works (deletes existing project first)\n- [ ] cmd/locdoc/add_test.go with integration tests\n- [ ] make validate passes","notes":"COMPLETED: AddCmd implementation with TDD\n- cmd/locdoc/add.go: Run method delegates to Crawler.CrawlProject\n- cmd/locdoc/add_test.go: 5 tests covering basic crawl, preview, force, filter validation\n- Preview mode shows URLs without creating project\n- Force mode deletes existing project first\n- Filter patterns validated early and stored in project\n- make validate passes","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-11T17:45:10.142185-08:00","updated_at":"2025-12-11T19:33:21.770034-08:00","closed_at":"2025-12-11T19:33:21.770037-08:00","dependencies":[{"issue_id":"locdoc-63p","depends_on_id":"locdoc-i7h","type":"parent-child","created_at":"2025-12-11T17:46:03.287673-08:00","created_by":"daemon"},{"issue_id":"locdoc-63p","depends_on_id":"locdoc-8cu","type":"blocks","created_at":"2025-12-11T17:48:03.945623-08:00","created_by":"daemon"},{"issue_id":"locdoc-63p","depends_on_id":"locdoc-t0c","type":"blocks","created_at":"2025-12-11T17:48:04.06887-08:00","created_by":"daemon"}]}
{"id":"locdoc-6af","title":"Wire both fetchers in main.go","description":"## Problem\nmain.go needs to create both HTTP and Rod fetchers and pass to Crawler.\n\n## Entrypoints\n- cmd/locdoc/main.go\n\n## Implementation\n```go\nhttpFetcher := http.NewFetcher(http.WithFetchTimeout(cli.Add.Timeout))\nrodFetcher, err := rod.NewFetcher(rod.WithFetchTimeout(cli.Add.Timeout))\n\ndeps.Crawler = \u0026crawl.Crawler{\n    HTTPFetcher: httpFetcher,\n    RodFetcher:  rodFetcher,\n    // ...\n}\n```\n\nEnsure both fetchers are closed on cleanup.\n\n## Validation\n- [ ] Both fetchers created with consistent timeout\n- [ ] Both passed to Crawler\n- [ ] Both closed on exit\n- [ ] make validate passes","notes":"Merged into locdoc-ytd - always probe simplification","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T14:59:02.431798-08:00","updated_at":"2025-12-20T18:25:31.025993-08:00","closed_at":"2025-12-20T18:25:31.025997-08:00","dependencies":[{"issue_id":"locdoc-6af","depends_on_id":"locdoc-0ox","type":"blocks","created_at":"2025-12-20T14:59:12.089213-08:00","created_by":"daemon"}]}
{"id":"locdoc-6ph","title":"Expand docs/ with workflow documentation","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-07T16:40:32.388874-08:00","updated_at":"2025-12-07T20:25:33.186919-08:00","closed_at":"2025-12-07T20:25:33.186923-08:00","dependencies":[{"issue_id":"locdoc-6ph","depends_on_id":"locdoc-hw3","type":"blocks","created_at":"2025-12-07T16:40:59.769594-08:00","created_by":"daemon"}]}
{"id":"locdoc-726","title":"Epic: Combine add and crawl commands","description":"Merge the two-step add/crawl flow into a single add command. Remove diffing logic in favor of simpler delete + recreate pattern. Add --force, --preview, and --filter flags.\n\nDesign doc: docs/plans/2025-12-10-combine-add-crawl-design.md","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T11:05:47.835912-08:00","updated_at":"2025-12-10T14:28:15.629078-08:00","closed_at":"2025-12-10T14:28:15.629081-08:00"}
{"id":"locdoc-76q","title":"Upgrade to Gemini 3 Flash","description":"## Problem\nGoogle released Gemini 3 Flash on Dec 17, 2025. It's faster and more capable than 2.5 Flash, with better agentic coding performance (78% on SWE-bench).\n\n## Changes\n- Update model constant from `gemini-2.5-flash` to `gemini-3-flash-preview`\n- Update README.md to reflect the new model\n\n## Entrypoints\n- `gemini/asker.go:12` - model constant\n- `cmd/locdoc/main.go:163` - defaultTokenizerModel constant\n- `README.md` - mentions \"Gemini 2.5 Flash\"\n\n## Validation\n- [ ] `locdoc ask` works with new model\n- [ ] README reflects Gemini 3 Flash\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T12:08:32.923093-08:00","updated_at":"2025-12-17T12:15:20.981192-08:00","closed_at":"2025-12-17T12:15:20.981195-08:00"}
{"id":"locdoc-77m","title":"Fix progress reporting for recursive crawling","description":"During recursive crawling, the progress display shows '[0/0]' which is misleading since we don't know the total URLs upfront.\n\n## Problem\nWhen crawling TanStack Query (recursive mode), output showed:\n```\n[0/0] .../latest/docs/framework/react/overview\n[0/0] ...s/framework/react/overview#motivation\n```\n\nThe counter stays at [0/0] because recursive crawling doesn't emit ProgressStarted with a total count.\n\n## Entrypoints\n- cmd/locdoc/add.go - progress callback formatting (lines 83-102)\n- crawl/crawl.go - recursiveCrawl progress events (lines 340-346, 426-431)\n\n## Approach\nOptions:\n1. Don't show [N/M] format for recursive crawling - just show the URL\n2. Show [N/?] or [N] format when total is unknown\n3. Have recursiveCrawl emit a different progress event type\n\n## Validation\n- Recursive crawling shows sensible progress (not [0/0])\n- Sitemap crawling still shows [N/M] format correctly\n- make validate passes","notes":"COMPLETED: Fixed progress reporting for recursive crawling\n- recursiveCrawl now sets Completed field in ProgressCompleted and ProgressFailed events\n- CLI progress callback now shows [N] format when total is unknown (0), [N/M] when known\n- Added tests for both crawl package and CLI behavior\n\nVALIDATION: make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T20:37:35.974005-08:00","updated_at":"2025-12-19T18:31:07.326115-08:00","closed_at":"2025-12-19T18:31:07.326118-08:00","dependencies":[{"issue_id":"locdoc-77m","depends_on_id":"locdoc-ksr","type":"blocks","created_at":"2025-12-18T20:37:39.981262-08:00","created_by":"daemon"}]}
{"id":"locdoc-7fc","title":"Consolidate remaining logging wrappers to slog package","description":"## Problem\nLogging wrappers are scattered across multiple packages:\n- `http/LoggingSitemapService` in http/logging.go\n- `goquery/LoggingRegistry` in goquery/logging.go\n\nPer Ben Johnson's Standard Package Layout, these should live in `slog/` since they depend on `log/slog`.\n\n## Background\nlocdoc-cqq established the `slog/` package pattern by moving `LoggingFetcher` there.\n\n## Entrypoints\n- http/logging.go - LoggingSitemapService\n- goquery/logging.go - LoggingRegistry\n- cmd/locdoc/main.go - debug mode wiring\n\n## Validation\n- [ ] LoggingSitemapService moved to slog/\n- [ ] LoggingRegistry moved to slog/\n- [ ] Imports updated in main.go\n- [ ] make validate passes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-21T09:34:13.935139-08:00","updated_at":"2025-12-21T09:46:43.156884-08:00","closed_at":"2025-12-21T09:46:43.156887-08:00"}
{"id":"locdoc-7n8","title":"Improve beads metadata workflow to avoid mixing with feature PRs","description":"## Problem\nWhen using `bd update` to mark tasks closed, the `.beads/issues.jsonl` change ends up in the working tree and can accidentally get included in feature PR commits. This causes PR review tools (like GitHub Copilot) to flag unintended changes.\n\n## Potential solutions\n- Commit beads changes separately before the feature commit\n- Add `.gitattributes` to mark `.beads/` as not needing review\n- Script the \"land this plane\" flow to handle beads commits first\n- Investigate if beads protected_branches config can help more","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-07T19:15:48.914404-08:00","updated_at":"2025-12-07T19:55:47.046488-08:00","closed_at":"2025-12-07T19:55:47.04649-08:00"}
{"id":"locdoc-7uh","title":"Delete obsolete tests and verify test reduction","description":"Delete tests for ParseAddArgs (Kong handles this). Delete tests that only verify argument parsing. Keep integration tests that verify end-to-end behavior. Verify total test line count is reduced by ~80%.","acceptance_criteria":"- [ ] ParseAddArgs tests deleted\n- [ ] Arg parsing edge case tests deleted\n- [ ] Integration tests remain and pass\n- [ ] Test line count reduced from ~1800 to ~300-400\n- [ ] make validate passes","notes":"COMPLETED: Deleted redundant tests (error path tests) from all command test files\nRESULT: Test lines reduced from ~1800 to 608 (66% reduction)\nKEPT: Essential integration tests - happy paths and distinct behaviors\nRATIONALE: 300-400 target would require removing tests for legitimate functionality","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T17:45:49.35442-08:00","updated_at":"2025-12-11T20:51:39.934423-08:00","closed_at":"2025-12-11T20:51:39.934425-08:00","dependencies":[{"issue_id":"locdoc-7uh","depends_on_id":"locdoc-i7h","type":"parent-child","created_at":"2025-12-11T17:46:03.453501-08:00","created_by":"daemon"},{"issue_id":"locdoc-7uh","depends_on_id":"locdoc-c1n","type":"blocks","created_at":"2025-12-11T17:48:04.23113-08:00","created_by":"daemon"}]}
{"id":"locdoc-7x3","title":"Consider SQLite WAL mode for write-heavy operations","description":"## Problem\n\nSQLite WAL (Write-Ahead Logging) mode can improve performance for write-heavy operations like crawling. Currently using default rollback journal.\n\n## Entrypoints\n\n- `sqlite/sqlite.go` - `Open()` method\n\n## Analysis Needed\n\n- Benchmark crawl performance with/without WAL\n- Consider trade-offs:\n  - WAL creates extra files (`-wal`, `-shm`) alongside database\n  - Better for concurrent access (less relevant for CLI)\n  - Better crash recovery\n  - Files must be checkpointed before moving database\n\n## References\n\n- https://github.com/benbjohnson/wtf/blob/main/sqlite/sqlite.go\n- https://sqlite.org/wal.html\n\n## Validation\n\n- [ ] Benchmark shows measurable improvement\n- [ ] Document decision either way","notes":"Implemented WAL mode. Benchmarks showed ~7x improvement for single inserts (51µs vs 344µs) and ~6.7x for bulk inserts (4.4ms vs 29ms for 100 docs). Also reduced memory allocations by ~20x per operation.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-08T13:20:05.747214-08:00","updated_at":"2025-12-09T19:23:57.450621-08:00","closed_at":"2025-12-09T19:23:57.450626-08:00"}
{"id":"locdoc-87y","title":"Fix preview mode reliability (concurrency + retries)","description":"## Problem\nPreview mode loses URLs due to:\n1. **High concurrency**: 10 workers overwhelm single Chrome browser, causing cascading timeouts\n2. **No retries**: Timeouts are permanent failures (unlike full crawl which retries 3x)\n\nObserved with TanStack docs: ~20 pages timed out and were lost.\n\n## Root Cause\n- `crawl/discover.go:56`: Intentionally skips retries 'to keep it fast'\n- `crawl/walk.go:62-64`: Defaults to 10 concurrent workers\n- Single Chrome process can't handle 10 heavy JS tabs\n\n## Proposed Solution\n1. Apply existing `-c` (concurrency) flag to preview mode\n2. Lower default preview concurrency (e.g., 3 instead of 10)\n3. Enable retries in preview mode (or add `--no-retry` to disable)\n\n## Entrypoints\n- crawl/discover.go (add retry logic)\n- cmd/locdoc/main.go (wire concurrency flag to preview)\n\n## Validation\n- Preview of heavy JS site completes without mass timeouts\n- `-c 3` works with `--preview`\n- make validate passes","notes":"COMPLETED: All implementation work\n- Added WithConcurrency and WithRetryDelays functional options to DiscoverURLs\n- Added retry logic using FetchWithRetryDelays\n- Lowered default concurrency from 10 to 3\n- Wired -c flag to preview mode\n- Added comprehensive tests for concurrency limiting, retry behavior, and defaults\n\nVALIDATION: make validate passes\n\nREADY FOR: /finish-task","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-20T09:58:37.133483-08:00","updated_at":"2025-12-20T10:25:10.363151-08:00","closed_at":"2025-12-20T10:25:10.363154-08:00"}
{"id":"locdoc-8bl","title":"Refactor DiscoverURLs to share implementation with recursiveCrawl","description":"## Problem\n`DiscoverURLs` (for preview mode) and `recursiveCrawl` duplicate significant logic:\n- Frontier management\n- Scope filtering (host, path prefix)\n- URL filter application\n- Rate limiting\n- Context cancellation handling\n\nThis duplication means:\n- Bug fixes need to be applied twice\n- Tests duplicate coverage (~255 lines in TestDiscoverURLs)\n- Concurrency improvements only benefit one path\n\n## Approach\nExtract shared logic into a common function (e.g., `walkFrontier`) that both can use. The difference is what happens with each page:\n- `DiscoverURLs`: just collect URLs\n- `recursiveCrawl`: extract content, save documents\n\n## Entrypoints\n- crawl/crawl.go: DiscoverURLs, recursiveCrawl\n\n## Validation\n- [ ] Shared implementation extracted\n- [ ] Both functions use shared code\n- [ ] Tests consolidated where possible\n- [ ] All existing tests pass\n- [ ] make validate passes","notes":"COMPLETED:\n- Extracted walkFrontier method that handles shared logic:\n  - Frontier management with Bloom filter deduplication\n  - Concurrent worker pool\n  - Work dispatch and result collection\n  - Scope filtering (delegated to handleResult)\n- Refactored recursiveCrawl to use walkFrontier (reduced from ~130 lines to ~25 lines)\n- Refactored DiscoverURLs to use walkFrontier (now concurrent, uses same machinery)\n- Both functions share the same concurrency infrastructure\n\nKEY CHANGES:\n- walkFrontier accepts walkProcessor and walkResultHandler callbacks\n- recursiveCrawl passes processRecursiveURL as processor\n- DiscoverURLs creates a minimal Crawler and uses simpler processor (no content extraction)\n- DiscoverURLs is now concurrent (was sequential before)\n\nFOLLOW-UP:\n- Created locdoc-tma for splitting crawl package into smaller files\n\nmake validate passes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-19T21:26:52.949965-08:00","updated_at":"2025-12-19T22:38:18.196457-08:00","closed_at":"2025-12-19T22:38:18.19646-08:00","dependencies":[{"issue_id":"locdoc-8bl","depends_on_id":"locdoc-jjg","type":"blocks","created_at":"2025-12-19T21:26:59.352528-08:00","created_by":"daemon"}]}
{"id":"locdoc-8cu","title":"Wire Dependencies struct and Kong binding","description":"Create Dependencies struct in main.go. Wire Kong to bind Dependencies to command Run methods. Initialize all services in Main.Run() and pass to parser. Update Main struct as needed.","acceptance_criteria":"- [ ] Dependencies struct with all fields (Ctx, Stdout, Stderr, DB, Projects, Documents, Sitemaps, Crawler, Asker)\n- [ ] Kong parser configured with kong.Bind(\u0026deps)\n- [ ] Services initialized before parsing\n- [ ] All commands receive Dependencies in Run method\n- [ ] make validate passes","notes":"COMPLETED: Wire Dependencies struct and Kong binding\n- Created Dependencies struct in cli.go with all fields (Ctx, Stdout, Stderr, DB, Projects, Documents, Sitemaps, Crawler, Asker)\n- Updated all command Run methods to accept *Dependencies parameter\n- Configured Kong parser with kong.Bind(deps) in Main.Run()\n- Initialize deps before parsing, populate with services after DB opens\n- Removed structural tests (reflection-based) that don't test behavior\n- Manual dispatch preserved until subsequent tasks implement Run methods\n- make validate passes","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-11T17:45:36.638079-08:00","updated_at":"2025-12-11T18:31:44.731014-08:00","closed_at":"2025-12-11T18:31:44.731017-08:00","dependencies":[{"issue_id":"locdoc-8cu","depends_on_id":"locdoc-i7h","type":"parent-child","created_at":"2025-12-11T17:46:03.404062-08:00","created_by":"daemon"},{"issue_id":"locdoc-8cu","depends_on_id":"locdoc-nle","type":"blocks","created_at":"2025-12-11T17:48:03.817055-08:00","created_by":"daemon"}]}
{"id":"locdoc-8pb","title":"Implement Bloom filter for URL deduplication","description":"Create bloom/ package wrapping github.com/bits-and-blooms/bloom/v3 for memory-efficient URL deduplication.\n\n## Entrypoints\n- Create bloom/filter.go - Filter struct with Add(), Test(), EstimatedCount()\n- Create bloom/filter_test.go\n\n## Validation\n- Filter correctly detects seen URLs\n- False positive rate ~1% at 10k URLs\n- make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T16:07:51.767495-08:00","updated_at":"2025-12-18T17:39:01.151928-08:00","closed_at":"2025-12-18T17:39:01.151931-08:00","dependencies":[{"issue_id":"locdoc-8pb","depends_on_id":"locdoc-2yj","type":"parent-child","created_at":"2025-12-18T16:16:39.481639-08:00","created_by":"daemon"},{"issue_id":"locdoc-8pb","depends_on_id":"locdoc-phn","type":"blocks","created_at":"2025-12-18T16:18:19.898311-08:00","created_by":"daemon"}]}
{"id":"locdoc-92d","title":"Implement DeleteCmd with Kong","description":"Create cmd/locdoc/delete.go with DeleteCmd struct and Run method. Requires --force flag. Finds project by name and deletes.","acceptance_criteria":"- [ ] cmd/locdoc/delete.go exists\n- [ ] DeleteCmd struct with Name arg and Force flag\n- [ ] Run method requires --force, deletes project\n- [ ] Error if project not found\n- [ ] cmd/locdoc/delete_test.go with integration tests\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T17:45:19.890186-08:00","updated_at":"2025-12-11T20:01:11.678528-08:00","closed_at":"2025-12-11T20:01:11.678531-08:00","dependencies":[{"issue_id":"locdoc-92d","depends_on_id":"locdoc-i7h","type":"parent-child","created_at":"2025-12-11T17:46:03.334153-08:00","created_by":"daemon"},{"issue_id":"locdoc-92d","depends_on_id":"locdoc-8cu","type":"blocks","created_at":"2025-12-11T17:48:03.995217-08:00","created_by":"daemon"}]}
{"id":"locdoc-9f6","title":"Add --filter flag to add command","description":"## Problem\nAdd --filter flag to add command that filters which URLs get crawled from the sitemap.\n\n## Requirements\n- **Regex patterns** (industry standard, already used in URLFilter)\n- **Multiple patterns via repeated flags**: `--filter \"docs\" --filter \"blog\"`\n- Patterns are OR'd together (URL passes if it matches ANY pattern)\n- Stored as newline-separated string in Project.Filter field\n- Converted to URLFilter.Include patterns at runtime\n\n## Implementation\n1. **Refactor first**: Extract CmdAdd flag parsing into `addOptions` struct + `parseAddArgs` function\n2. Add --filter to the options struct ([]string for multiple values)\n3. Pass filters to DiscoverURLs in preview mode\n4. Store filters on project creation\n\n## Entrypoints\n- cmd/locdoc/main.go (CmdAdd)\n- locdoc.go (URLFilter already exists)\n\n## Validation\n- [ ] CmdAdd uses addOptions struct for clean parsing\n- [ ] --filter accepts regex pattern and stores on project\n- [ ] Multiple --filter flags can be specified\n- [ ] --preview --filter shows only matching URLs  \n- [ ] Crawl only fetches URLs matching at least one pattern\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T11:06:17.781395-08:00","updated_at":"2025-12-10T12:43:23.602359-08:00","closed_at":"2025-12-10T12:43:23.602361-08:00","dependencies":[{"issue_id":"locdoc-9f6","depends_on_id":"locdoc-tsu","type":"blocks","created_at":"2025-12-10T11:06:44.437565-08:00","created_by":"daemon"}]}
{"id":"locdoc-9h9","title":"Add DocumentFormatter to root package","description":"## Problem\n\nNeed a shared component to format documents for display and LLM context.\n\n## Entrypoints\n\n- Create new file `formatter.go` in root package\n\n## Implementation\n\n```go\n// DocumentFormatter formats documents for display or LLM context.\ntype DocumentFormatter interface {\n    Format(docs []*Document) string\n}\n```\n\nOr simpler, just a function type / concrete implementation.\n\nOutput format:\n```\n## Document: {title or source URL}\n{content}\n\n## Document: {title or source URL}\n{content}\n```\n\n- Use title if available, fall back to source URL\n- Documents separated by blank lines\n- Preserve markdown content as-is\n\n## Usage\n\n- `locdoc docs` prints this to stdout\n- `locdoc ask` wraps this in prompt template for Gemini\n\n## Validation\n\n- [ ] Formatter defined\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-09T18:00:37.792739-08:00","updated_at":"2025-12-09T19:44:53.984031-08:00","closed_at":"2025-12-09T19:44:53.984035-08:00","dependencies":[{"issue_id":"locdoc-9h9","depends_on_id":"locdoc-296","type":"blocks","created_at":"2025-12-09T18:00:42.552411-08:00","created_by":"daemon"}]}
{"id":"locdoc-9ix","title":"Add live progress reporting during crawl","description":"## Problem\nLong crawls (10+ minutes) show nothing until completion, leaving users unsure if the process is working.\n\n## Entrypoints\n- `cmd/locdoc/main.go` - `crawlProject()`\n\n## Validation\n- [ ] Progress line shows: `[N/M] url (X failed, Y saved, ~Zk tokens)`\n- [ ] Progress updates in place using carriage return\n- [ ] Failures print on separate lines (persist in scroll history)\n- [ ] Progress updates as URLs complete (not batched at end)\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T15:41:17.7157-08:00","updated_at":"2025-12-10T18:24:54.685912-08:00","closed_at":"2025-12-10T18:24:54.685915-08:00"}
{"id":"locdoc-a3x","title":"Move retry logic to crawl package","description":"Move FetchWithRetry and related code from cmd/locdoc/retry.go to crawl/retry.go. Move tests to crawl/retry_test.go. Update any imports. Delete old files.","acceptance_criteria":"- [ ] crawl/retry.go contains retry logic\n- [ ] crawl/retry_test.go contains retry tests\n- [ ] cmd/locdoc/retry.go deleted\n- [ ] cmd/locdoc/retry_test.go deleted  \n- [ ] All retry tests pass\n- [ ] make validate passes","notes":"COMPLETED: Moved retry logic from cmd/locdoc to crawl package\n- crawl/retry.go: FetchWithRetry, FetchWithRetryDelays, DefaultRetryDelays, FetchFunc, LogFunc\n- crawl/retry_test.go: All unit tests\n- crawl/retry_integration_test.go: Integration test for exponential backoff\n- Updated cmd/locdoc/main.go to import and use crawl package\n- Deleted cmd/locdoc/retry.go and cmd/locdoc/retry_test.go\n\nVALIDATION: make validate passes (0 lint issues, all tests pass)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T17:44:57.545892-08:00","updated_at":"2025-12-11T18:48:10.115262-08:00","closed_at":"2025-12-11T18:48:10.115265-08:00","dependencies":[{"issue_id":"locdoc-a3x","depends_on_id":"locdoc-i7h","type":"parent-child","created_at":"2025-12-11T17:46:03.241017-08:00","created_by":"daemon"},{"issue_id":"locdoc-a3x","depends_on_id":"locdoc-fyy","type":"blocks","created_at":"2025-12-11T17:48:03.864296-08:00","created_by":"daemon"}]}
{"id":"locdoc-a3y","title":"Epic: Documentation Crawler MVP","description":"## Overview\n\nImplement pure Go documentation crawler as defined in docs/plans/2025-12-07-crawling-design.md.\n\n## Scope\n\n- Sitemap-based URL discovery\n- Browser-based page fetching (JS rendering)\n- Content extraction and markdown conversion\n- SQLite storage\n\n## Success Criteria\n\n- `locdoc add \u003cname\u003e \u003curl\u003e` registers project\n- `locdoc crawl [name]` fetches pages from sitemap, stores as Markdown\n- `make validate` passes\n- Builds with `CGO_ENABLED=0`\n\n## Implementation Order\n\n1. sqlite/ - Storage foundation\n2. http/ - Sitemap parsing\n3. rod/ - Browser fetching\n4. trafilatura/ - Content extraction\n5. htmltomd/ - Markdown conversion\n6. cmd/locdoc/ - CLI wiring","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-07T22:00:37.291952-08:00","updated_at":"2025-12-09T14:26:04.271553-08:00","closed_at":"2025-12-09T14:26:04.271555-08:00"}
{"id":"locdoc-a4x","title":"Implement rod/ package","description":"## Problem\n\nNeed browser-based page fetching for JS-rendered documentation sites.\n\n## Entrypoints\n\n- Create `rod/` directory\n- Implement fetcher that returns rendered HTML\n\n## Requirements\n\n- Use `go-rod/rod` for Chrome automation\n- Launch Chrome or connect to existing instance\n- Navigate to URL, wait for JS to render\n- Return rendered HTML as string\n- Handle timeouts gracefully\n- Clean up browser resources on shutdown\n- Define interface in root package during implementation\n\n## Validation\n\n- [ ] Unit tests with mock browser (if feasible)\n- [ ] Integration test against real JS-heavy site\n- [ ] Verify Chrome requirement is documented\n- [ ] `make validate` passes\n\n## References\n\n- docs/plans/2025-12-07-crawling-design.md\n- go-rod/rod documentation","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-07T22:01:24.611936-08:00","updated_at":"2025-12-09T04:54:24.256482-08:00","closed_at":"2025-12-09T04:54:24.256485-08:00","dependencies":[{"issue_id":"locdoc-a4x","depends_on_id":"locdoc-k4l","type":"blocks","created_at":"2025-12-07T22:01:57.221765-08:00","created_by":"daemon"},{"issue_id":"locdoc-a4x","depends_on_id":"locdoc-a3y","type":"parent-child","created_at":"2025-12-07T22:02:04.977866-08:00","created_by":"daemon"}]}
{"id":"locdoc-a6b","title":"Epic: Adaptive rendering detection","description":"## Problem\nlocdoc uses Rod (headless Chrome) for all page fetching. This is slow and resource-intensive. 70-80% of doc sites can be fetched with HTTP-only.\n\n## Solution\nProbe first URL to determine rendering strategy for entire site. Use HTTP when possible, Rod when JS required.\n\n## Design\nSee docs/plans/2025-01-20-adaptive-rendering-design.md","notes":"Epic completed. All components implemented:\n- RequiresJS() method in goquery/detector.go\n- HTTP Fetcher in http/fetcher.go  \n- ContentDiffers() in crawl/compare.go\n- Probe logic in crawl/crawl.go and crawl/discover.go\n- Both fetchers wired in cmd/locdoc/main.go\n\nAdaptive rendering detection now probes the first URL and uses HTTP-only fetching when JavaScript is not required.","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T14:50:59.581883-08:00","updated_at":"2025-12-21T09:54:31.958255-08:00","closed_at":"2025-12-21T09:54:31.958258-08:00","dependencies":[{"issue_id":"locdoc-a6b","depends_on_id":"locdoc-6af","type":"blocks","created_at":"2025-12-20T14:59:15.820998-08:00","created_by":"daemon"}]}
{"id":"locdoc-a7o","title":"Add --help flag to CLI","description":"## Problem\n\nCLI does not support --help or -h flags. Running 'locdoc --help' returns 'unknown command' error.\n\n## Entrypoints\n\n- cmd/locdoc/main.go\n\n## Implementation\n\nAdd case for 'help', '--help', '-h' in command switch that calls usage() without returning an error.\n\n## Validation\n\n- [ ] locdoc --help shows usage\n- [ ] locdoc -h shows usage\n- [ ] locdoc help shows usage\n- [ ] make validate passes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-09T20:55:14.222704-08:00","updated_at":"2025-12-09T21:17:13.935164-08:00","closed_at":"2025-12-09T21:17:13.935168-08:00"}
{"id":"locdoc-aee","title":"Add section extraction for prompt enrichment","description":"## Problem\nThe Gemini prompt could benefit from section metadata to help cite more precisely. Rather than storing sections in the database (adds complexity), extract them on-the-fly when building the prompt.\n\n## Design Decisions\n\n**Location**: Root package (section.go)\n- Section is a pure domain type\n- ExtractSections is pure domain logic (no external deps)\n- Per go-standard-package-layout, belongs in root\n\n**No interface needed**:\n- ExtractSections is a pure function: `string → []Section`\n- No state, no side effects, deterministic\n- Nothing to mock - test with real markdown input\n- Only one sensible implementation (regex-based)\n\n## Implementation\n\n1. Add to root package (section.go):\n```go\ntype Section struct {\n    Level  int    `json:\"level\"`\n    Title  string `json:\"title\"`\n    Anchor string `json:\"anchor\"`\n}\n\nfunc ExtractSections(markdown string) []Section\n```\n\n2. Implementation details:\n   - Parse H1-H6 markdown headings via regex\n   - Generate URL-safe anchors (lowercase, hyphenated)\n   - Handle duplicate headings with numeric suffixes (example, example-1, example-2)\n\n3. Update gemini/asker.go BuildUserPrompt:\n   - Call `locdoc.ExtractSections(doc.Content)` for each document\n   - Include section titles in XML structure\n\n## Entrypoints\n- Create section.go in root package\n- gemini/asker.go:78-103 (BuildUserPrompt)\n\n## Validation\n- [ ] ExtractSections parses H1-H6 headings correctly\n- [ ] Anchors are URL-safe (lowercase, hyphenated)\n- [ ] Duplicate headings get unique anchors (example, example-1)\n- [ ] Empty markdown returns empty slice\n- [ ] BuildUserPrompt includes section list per document\n- [ ] make validate passes","notes":"COMPLETED: ExtractSections in section.go, tests, BuildUserPrompt integration\nVALIDATION: make validate passes\nREADY: For code review","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-21T11:49:23.551665-08:00","updated_at":"2025-12-21T11:57:31.757915-08:00","closed_at":"2025-12-21T11:57:31.757918-08:00"}
{"id":"locdoc-amg","title":"Add Asker interface to root package","description":"## Problem\n\nNeed a domain interface for asking natural language questions about documentation.\n\n## Entrypoints\n\n- Create new file `asker.go` in root package\n\n## Implementation\n\n```go\n// Asker provides natural language question answering over documentation.\ntype Asker interface {\n    Ask(ctx context.Context, projectID string, question string) (string, error)\n}\n```\n\n## Validation\n\n- [ ] Interface defined in root package\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-09T16:28:46.880379-08:00","updated_at":"2025-12-09T20:26:16.589741-08:00","closed_at":"2025-12-09T20:26:16.589744-08:00"}
{"id":"locdoc-axo","title":"Use Gemini system instruction API field","description":"## Problem\nMove system instruction from inline prompt to the API's SystemInstruction field and add temperature config.\n\n## Entrypoints\n- gemini/asker.go: Ask method, add GenerateContentConfig\n\n## Validation\n- [ ] System instruction passed via genai.GenerateContentConfig.SystemInstruction\n- [ ] Temperature set to 0.4 via config\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T21:02:01.853053-08:00","updated_at":"2025-12-10T21:08:00.066423-08:00","closed_at":"2025-12-10T21:08:00.066428-08:00"}
{"id":"locdoc-b1k","title":"Generic link selector fails on Tailwind CSS sites","description":"## Problem\nPreview mode finds 0 links on TanStack docs (https://tanstack.com/query/v5/docs) even though there are 240 docs links on the page.\n\n## Root Cause\nThe GenericSelector looks for semantic HTML elements:\n- `nav`, `aside`, `main`, `article`, `footer`\n- `.sidebar`, `.toc`, `.menu`, `.content`\n- `role=\"navigation\"`\n\nTanStack uses Tailwind CSS with no semantic HTML - all navigation is in `\u003cdiv\u003e` elements with utility classes like `flex flex-col gap-4`.\n\n## Evidence\n```\nnav elements: 1 (only 3 links, not docs)\naside elements: 0\nmain elements: 0\nTotal a[href]: 349\nLinks to /query/v5/docs: 240\n```\n\n## Proposed Fix\nAdd fallback logic: if no links found by semantic selectors, extract ALL internal links from the page. This makes preview mode work for any site structure.\n\n## Validation\n- `locdoc add --preview testdocs https://tanstack.com/query/v5/docs` returns 240+ URLs\n- make validate passes","notes":"COMPLETED: Implemented fallback link extraction for sites without semantic HTML\n- Added PriorityFallback (10) to link priorities \n- GenericSelector now always extracts all internal links with fallback priority\n- Links found via semantic selectors keep higher priority due to deduplication\n- TanStack docs preview now finds 279+ URLs (vs 0 before)\n\nVALIDATED: make validate passes, manual test against TanStack docs successful","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-19T19:57:40.026807-08:00","updated_at":"2025-12-19T23:53:28.62161-08:00","closed_at":"2025-12-19T23:53:28.621614-08:00"}
{"id":"locdoc-bhp","title":"Implement ListCmd with Kong","description":"Create cmd/locdoc/list.go with ListCmd struct and Run method. Simple command - just calls ProjectService.FindProjects and formats output.","acceptance_criteria":"- [ ] cmd/locdoc/list.go exists\n- [ ] ListCmd struct (no arguments)\n- [ ] Run method lists projects with ID, name, URL\n- [ ] Shows helpful message when no projects exist\n- [ ] cmd/locdoc/list_test.go with integration tests\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T17:45:14.664954-08:00","updated_at":"2025-12-11T19:50:36.723161-08:00","closed_at":"2025-12-11T19:50:36.723164-08:00","dependencies":[{"issue_id":"locdoc-bhp","depends_on_id":"locdoc-i7h","type":"parent-child","created_at":"2025-12-11T17:46:03.310929-08:00","created_by":"daemon"},{"issue_id":"locdoc-bhp","depends_on_id":"locdoc-8cu","type":"blocks","created_at":"2025-12-11T17:48:03.970589-08:00","created_by":"daemon"}]}
{"id":"locdoc-bky","title":"Implement HTTP fetcher","description":"## Problem\nNeed a simple HTTP-based fetcher as alternative to Rod for static sites.\n\n## Entrypoints\n- http/fetcher.go (new file)\n\n## Implementation\n```go\ntype Fetcher struct {\n    client  *http.Client\n    timeout time.Duration\n}\n\nfunc NewFetcher(opts ...Option) *Fetcher  // Default 10s timeout\nfunc (f *Fetcher) Fetch(ctx context.Context, url string) (string, error)\nfunc (f *Fetcher) Close() error  // No-op\n```\n\n- 10s default timeout (consistent with Rod)\n- Implements locdoc.Fetcher interface\n- Uses existing retry infrastructure from crawl/retry.go\n\n## Validation\n- [ ] Fetcher returns HTML body as string\n- [ ] Timeout is respected\n- [ ] httptest.Server based tests\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T14:51:14.156626-08:00","updated_at":"2025-12-20T15:26:12.350603-08:00","closed_at":"2025-12-20T15:26:12.350606-08:00"}
{"id":"locdoc-bqz","title":"Remove UpdateDocument and document update logic","description":"## Problem\n\nWith the new add/force workflow, documents are always created fresh - never updated:\n- New projects have no existing documents\n- Re-crawling requires --force which deletes the project (and all docs via CASCADE)\n\nThe UpdateDocument functionality is now dead code.\n\n## Changes\n\n1. Remove UpdateDocument from DocumentService interface (document.go)\n2. Remove DocumentUpdate struct (document.go)\n3. Remove UpdateDocument implementation (sqlite/document.go)\n4. Remove UpdateDocument tests (sqlite/document_test.go)\n5. Remove UpdateDocumentFn from mock (mock/document.go)\n6. Simplify crawlProject to only create documents (cmd/locdoc/main.go)\n7. Remove related tests in main_test.go\n\n## Entrypoints\n\n- document.go - interface and DocumentUpdate struct\n- sqlite/document.go - implementation\n- cmd/locdoc/main.go - crawlProject function\n\n## Validation\n\n- [ ] UpdateDocument removed from interface\n- [ ] DocumentUpdate struct removed\n- [ ] SQLite implementation removed\n- [ ] Mock updated\n- [ ] crawlProject simplified (no update branches)\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T14:30:10.333504-08:00","updated_at":"2025-12-10T14:49:37.010315-08:00","closed_at":"2025-12-10T14:49:37.010319-08:00"}
{"id":"locdoc-bxd","title":"Add mock implementation docs to go-standard-package-layout skill","description":"## Problem\n\nThe go-standard-package-layout skill doesn't document how to implement mocks following Ben Johnson's pattern.\n\n## Entrypoints\n\n- `/Users/filip/.claude/skills/go-standard-package-layout/go-standard-package-layout.md`\n\n## Requirements\n\nAdd a section on mock implementation patterns:\n- One file per entity in mock/ package (e.g., mock/user.go, mock/project.go)\n- Function fields pattern for dependency injection\n- Compile-time interface verification\n- Example from wtf repo\n\n## Validation\n\n- [ ] Skill file updated with mock section\n- [ ] Examples included","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-09T14:25:34.540459-08:00","updated_at":"2025-12-09T14:50:18.181082-08:00","closed_at":"2025-12-09T14:50:18.181087-08:00"}
{"id":"locdoc-c1n","title":"Remove old command functions and clean up main.go","description":"Delete old CmdAdd, CmdList, CmdDelete, CmdDocs, CmdAsk functions. Delete ParseAddArgs. Delete CrawlDeps, AddOptions. Delete crawlProject, processURL. Clean up imports. Main.go should only contain Main struct, CLI struct, Dependencies, and wiring.","acceptance_criteria":"- [ ] All Cmd* functions deleted\n- [ ] ParseAddArgs deleted\n- [ ] CrawlDeps, AddOptions deleted\n- [ ] crawlProject, processURL deleted\n- [ ] main.go is ~100-150 lines\n- [ ] make validate passes\n- [ ] All existing tests pass (via new implementation)","notes":"COMPLETED: All old command functions deleted\n- Removed CmdAdd, CmdList, CmdDelete, CmdDocs, CmdAsk\n- Removed ParseAddArgs, AddOptions, CrawlDeps, crawlResult\n- Removed crawlProject, processURL, ComputeHashForTest\n- Removed runAdd, runList, runDelete, runDocs, runAsk methods\n- Updated Run() to use Kong's Parse + Run dispatch\n- Cleaned up imports\n- main.go now 173 lines (was 822)\n- main_test.go now 92 lines (was 1618)\n- All tests pass, make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T17:45:43.304843-08:00","updated_at":"2025-12-11T20:40:42.433484-08:00","closed_at":"2025-12-11T20:40:42.433487-08:00","dependencies":[{"issue_id":"locdoc-c1n","depends_on_id":"locdoc-i7h","type":"parent-child","created_at":"2025-12-11T17:46:03.427792-08:00","created_by":"daemon"},{"issue_id":"locdoc-c1n","depends_on_id":"locdoc-63p","type":"blocks","created_at":"2025-12-11T17:48:04.096532-08:00","created_by":"daemon"},{"issue_id":"locdoc-c1n","depends_on_id":"locdoc-bhp","type":"blocks","created_at":"2025-12-11T17:48:04.123863-08:00","created_by":"daemon"},{"issue_id":"locdoc-c1n","depends_on_id":"locdoc-92d","type":"blocks","created_at":"2025-12-11T17:48:04.15076-08:00","created_by":"daemon"},{"issue_id":"locdoc-c1n","depends_on_id":"locdoc-o5n","type":"blocks","created_at":"2025-12-11T17:48:04.176131-08:00","created_by":"daemon"},{"issue_id":"locdoc-c1n","depends_on_id":"locdoc-c7n","type":"blocks","created_at":"2025-12-11T17:48:12.545202-08:00","created_by":"daemon"}]}
{"id":"locdoc-c4s","title":"Create mock package skeleton","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-07T16:40:32.293012-08:00","updated_at":"2025-12-07T19:12:29.650324-08:00","closed_at":"2025-12-07T19:12:29.650327-08:00","dependencies":[{"issue_id":"locdoc-c4s","depends_on_id":"locdoc-1h6","type":"blocks","created_at":"2025-12-07T16:41:00.022992-08:00","created_by":"daemon"}]}
{"id":"locdoc-c7n","title":"Implement AskCmd with Kong","description":"Create cmd/locdoc/ask.go with AskCmd struct and Run method. Takes project name and question, uses Asker service to get answer.","acceptance_criteria":"- [ ] cmd/locdoc/ask.go exists\n- [ ] AskCmd struct with Name and Question args\n- [ ] Run method calls Asker.Ask and prints result\n- [ ] Error if project not found\n- [ ] cmd/locdoc/ask_test.go with integration tests\n- [ ] make validate passes","notes":"COMPLETED: Implemented AskCmd with Kong\n- Created ask.go with Run method\n- Created ask_test.go with 4 test cases (happy path, project not found, FindProjects error, Asker error)\n- Removed stub Run method from cli.go\n- make validate passes\n\nAll acceptance criteria met:\n- cmd/locdoc/ask.go exists ✓\n- AskCmd struct with Name and Question args ✓ (already in cli.go)\n- Run method calls Asker.Ask and prints result ✓\n- Error if project not found ✓\n- cmd/locdoc/ask_test.go with integration tests ✓\n- make validate passes ✓","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T17:45:30.439382-08:00","updated_at":"2025-12-11T20:19:08.998186-08:00","closed_at":"2025-12-11T20:19:08.998189-08:00","dependencies":[{"issue_id":"locdoc-c7n","depends_on_id":"locdoc-i7h","type":"parent-child","created_at":"2025-12-11T17:46:03.380711-08:00","created_by":"daemon"},{"issue_id":"locdoc-c7n","depends_on_id":"locdoc-8cu","type":"blocks","created_at":"2025-12-11T17:48:04.043334-08:00","created_by":"daemon"}]}
{"id":"locdoc-co5","title":"Add --force flag to add command","description":"Add --force flag that deletes existing project before creating new one.\n\n## Behavior\n- Without --force: error if project exists\n- With --force: delete existing project + docs, then create + crawl\n- Enables re-crawling via delete + recreate pattern\n\n## Entrypoints\n- cmd/locdoc/main.go (CmdAdd)\n\n## Validation\n- [ ] --force deletes existing and recreates\n- [ ] Without --force, error on existing project\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T11:06:29.922497-08:00","updated_at":"2025-12-10T13:46:00.791702-08:00","closed_at":"2025-12-10T13:46:00.791704-08:00","dependencies":[{"issue_id":"locdoc-co5","depends_on_id":"locdoc-2zl","type":"blocks","created_at":"2025-12-10T11:06:44.573399-08:00","created_by":"daemon"}]}
{"id":"locdoc-cqq","title":"Add logging wrapper for HTTP fetcher","description":"## Problem\nIn debug mode, RodFetcher is wrapped with LoggingFetcher but HTTPFetcher is not. This creates inconsistent logging - HTTP fetches aren't logged even when debugging.\n\n## Options\n1. Create separate `http.NewLoggingFetcher` mirroring rod.NewLoggingFetcher\n2. Move LoggingFetcher to a shared location (root package or separate package) so both fetchers can use it\n\nOption 2 is cleaner - the logging wrapper is generic and doesn't depend on rod internals.\n\n## Entrypoints\n- rod/logging.go - current LoggingFetcher implementation\n- cmd/locdoc/main.go:148-152 - debug mode wiring\n\n## Validation\n- [ ] HTTPFetcher wrapped with logging in debug mode\n- [ ] Both fetchers log consistently\n- [ ] make validate passes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-20T19:07:46.250756-08:00","updated_at":"2025-12-21T09:19:58.696397-08:00","closed_at":"2025-12-21T09:19:58.696401-08:00"}
{"id":"locdoc-d01","title":"Extract section metadata and anchors from documents","description":"## Problem\nDocuments are stored as monolithic markdown blobs. We can't:\n- Cite specific sections (\"see Authentication \u003e OAuth2\")\n- Generate anchor links (#oauth2-setup)\n- Help Gemini understand document structure\n\n## Approach\n1. During markdown conversion, extract heading structure\n2. Generate anchors from heading text (\"OAuth2 Setup\" → `#oauth2-setup`)\n3. Store as JSON in new `sections` field\n4. Include section list in Gemini prompt for retrieval precision\n5. Output citations as URLs with anchors\n\n## Data Model Changes\nAdd to Document struct and SQLite schema:\n```go\nSections string // JSON: [{\"level\": 2, \"title\": \"OAuth2\", \"anchor\": \"oauth2\"}, ...]\n```\n\n## Implementation\n1. Add section extraction to htmltomarkdown conversion (or post-process markdown)\n2. Add `sections` column to documents table\n3. Update crawl to populate sections field\n4. Update BuildUserPrompt to include section list per document\n5. Update citation instructions to use URL#anchor format\n\n## Entrypoints\n- document.go (Document struct)\n- sqlite/sqlite.go (schema)\n- htmltomarkdown/converter.go or new section extractor\n- gemini/asker.go:78-93 (BuildUserPrompt)\n\n## Validation\n- [ ] Sections extracted from sample docs with headings\n- [ ] Anchors generated consistently (lowercase, hyphenated)\n- [ ] Gemini prompt includes section metadata\n- [ ] Citations can include #anchor when relevant\n- [ ] make validate passes","notes":"Decided against storing sections in data model. Keeping pages as discrete docs maintains simplicity and future flexibility (e.g., file-based storage). Section extraction can happen on-the-fly at prompt-build time instead.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-21T11:38:16.310884-08:00","updated_at":"2025-12-21T11:47:57.985612-08:00","closed_at":"2025-12-21T11:47:57.985615-08:00"}
{"id":"locdoc-d6s","title":"Implement generic fallback link selector","description":"Create generic selector using universal CSS selectors that work across any documentation framework.\n\n## Entrypoints\n- Create goquery/selector_generic.go - GenericSelector using nav, aside, .sidebar, etc.\n- Create goquery/selector_generic_test.go\n\n## Validation\n- Extracts navigation links from arbitrary HTML\n- Priority: TOC \u003e nav \u003e content \u003e footer\n- make validate passes","notes":"COMPLETED: GenericSelector implementation with TDD\n- Created goquery/selector_generic.go with universal CSS selectors\n- Created goquery/selector_generic_test.go with 12 test cases\n- Selectors: .toc, .sidebar, .table-of-contents, aside (TOC priority)\n- Selectors: nav, [role=navigation], .nav, .menu, .navbar (nav priority)\n- Selectors: main, article, .content, .doc-content (content priority)\n- Selectors: footer, .footer (footer priority)\n- All tests pass, make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T16:07:52.519571-08:00","updated_at":"2025-12-18T17:20:15.013916-08:00","closed_at":"2025-12-18T17:20:15.013919-08:00","dependencies":[{"issue_id":"locdoc-d6s","depends_on_id":"locdoc-2yj","type":"parent-child","created_at":"2025-12-18T16:16:39.604457-08:00","created_by":"daemon"},{"issue_id":"locdoc-d6s","depends_on_id":"locdoc-nwx","type":"blocks","created_at":"2025-12-18T16:18:22.623627-08:00","created_by":"daemon"}]}
{"id":"locdoc-ddp","title":"Add type-safe DocumentFilter.SortBy","description":"## Problem\nDocumentFilter.SortBy is a plain string that accepts 'position' or 'fetched_at', but there's no type safety - misspellings like 'postition' would silently use the default.\n\n## Approach\n1. Create SortOrder type: `type SortOrder string`\n2. Define constants: SortByPosition, SortByFetchedAt\n3. Update DocumentFilter to use SortOrder type\n4. Update sqlite/document.go switch statement\n\n## Entrypoints\n- document.go:53-62 (DocumentFilter struct)\n- sqlite/document.go:111-116 (switch statement)\n\n## Validation\n- [ ] Existing document tests pass\n- [ ] No behavior changes\n- [ ] make validate passes","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-21T10:05:48.155762-08:00","updated_at":"2025-12-21T10:05:48.155762-08:00"}
{"id":"locdoc-ecb","title":"Epic: Ask Command","description":"## Overview\n\nImplement the `locdoc ask \u003cproject\u003e \"question\"` command that queries documentation using Gemini Flash.\n\n## Design\n\nSee docs/plans/2025-12-09-ask-command-design.md\n\n## Scope\n\n- Add `Asker` interface to root package\n- Implement `gemini/` package wrapping Gemini API\n- Add `CmdAsk` to CLI\n- LLM-friendly error messages\n\n## Validation\n\n- [ ] `locdoc ask \u003cproject\u003e \"question\"` returns useful answers\n- [ ] Error messages guide users to correct usage\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-09T16:28:42.085842-08:00","updated_at":"2025-12-09T21:27:11.834105-08:00","closed_at":"2025-12-09T21:27:11.83411-08:00","dependencies":[{"issue_id":"locdoc-ecb","depends_on_id":"locdoc-il8","type":"blocks","created_at":"2025-12-09T16:29:04.693172-08:00","created_by":"daemon"}]}
{"id":"locdoc-ez3","title":"Add --debug flag for preview command","description":"## Problem\nDuring preview, there's no visibility into what's happening. The command appears to hang while Chromium is working in the background.\n\n## Proposed Solution\nAdd a `--debug` flag to the preview command that logs progress information:\n- Pages being fetched\n- Links being discovered\n- Framework detection results\n- Timing information\n\n## Entrypoints\n- cmd/locdoc/main.go (add flag)\n- Relevant crawl/preview logic\n\n## Validation\n- `locdoc add --preview --debug \u003cname\u003e \u003curl\u003e` shows progress logs\n- Normal mode (without --debug) remains quiet\n- make validate passes","notes":"COMPLETED: Implementation of --debug flag\n\nImplementation:\n- Added Debug bool to AddCmd in cli.go\n- Created logging decorators using go-kit pattern with slog:\n  - http/logging.go: LoggingSitemapService\n  - rod/logging.go: LoggingFetcher  \n  - goquery/logging.go: LoggingRegistry\n- All decorators log duration for performance debugging\n- Wired in main.go when --debug is set\n\nTests:\n- Unit tests for each decorator\n- Integration tests in add_test.go verifying:\n  - Debug mode logs to stderr\n  - Non-debug mode remains quiet\n\nSelf-review addressed: Added missing integration tests\n\nREADY FOR: /finish-task","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-19T23:52:27.632682-08:00","updated_at":"2025-12-20T09:46:25.976249-08:00","closed_at":"2025-12-20T09:46:25.976252-08:00"}
{"id":"locdoc-fps","title":"Consider DiscoverURLs API redesign","description":"## Problem\nDiscoverURLs now takes many parameters making calls verbose:\n```go\nfunc DiscoverURLs(ctx, sourceURL, urlFilter, linkSelectors, rateLimiter, httpFetcher, rodFetcher, prober, extractor, opts...)\n```\n\n## Proposal\nConsider redesigning as a struct with method:\n```go\ntype Discoverer struct {\n    LinkSelectors locdoc.LinkSelectorRegistry\n    RateLimiter   locdoc.DomainLimiter\n    HTTPFetcher   locdoc.Fetcher\n    RodFetcher    locdoc.Fetcher\n    Prober        locdoc.Prober\n    Extractor     locdoc.Extractor\n}\n\nfunc (d *Discoverer) DiscoverURLs(ctx, sourceURL, urlFilter, opts...) ([]string, error)\n```\n\n## Entrypoints\n- crawl/discover.go\n\n## Validation\n- [ ] API is more ergonomic for callers\n- [ ] make validate passes","notes":"COMPLETED: Full API redesign with cleanup\n\nCHANGES:\n1. Converted DiscoverURLs from standalone function to Crawler method\n   - Before: crawl.DiscoverURLs(ctx, url, filter, linkSelectors, rateLimiter, httpFetcher, rodFetcher, prober, extractor, opts...)\n   - After: crawler.DiscoverURLs(ctx, url, filter, opts...)\n\n2. Simplified add.go caller to use deps.Crawler.DiscoverURLs()\n\n3. Removed redundant Dependencies fields (LinkSelectors, RateLimiter, HTTPFetcher, RodFetcher, Prober, Extractor)\n\n4. Restructured main.go to always create deps.Crawler for both preview and non-preview modes\n\n5. Updated all tests in discover_test.go and add_test.go\n\nVALIDATION: make validate passes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-20T18:18:07.640902-08:00","updated_at":"2025-12-21T07:54:30.018513-08:00","closed_at":"2025-12-21T07:54:30.018516-08:00"}
{"id":"locdoc-fx7","title":"Sandwich pattern with structured sources output","description":"## Problem\nAdd trailing instructions after the question (sandwich pattern) and require structured Sources section in output.\n\n## Entrypoints\n- gemini/asker.go: buildPrompt/buildUserMessage function\n\n## Validation\n- [ ] Question wrapped in \u003cquestion\u003e tags\n- [ ] Trailing \u003cinstructions\u003e block after question\n- [ ] Instructions specify Sources format: ---\\nSources:\\n- url1\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T21:02:16.778964-08:00","updated_at":"2025-12-10T21:21:45.970054-08:00","closed_at":"2025-12-10T21:21:45.970057-08:00"}
{"id":"locdoc-fyy","title":"Create crawl package with Crawler struct and types","description":"Create crawl/ package with Crawler struct, Result type, ProgressEvent type, ProgressFunc type. No implementation yet - just the API surface. Include compile-time interface check placeholder.","acceptance_criteria":"- [ ] crawl/crawl.go exists with Crawler struct\n- [ ] All dependency fields on Crawler (Sitemaps, Fetcher, etc.)\n- [ ] Result, ProgressEvent, ProgressType, ProgressFunc types defined\n- [ ] CrawlProject method signature (can return nil, nil for now)\n- [ ] make validate passes","notes":"COMPLETED: Created crawl package with API surface\n- crawl/crawl.go: Crawler struct with all dependency fields\n- Result, ProgressEvent, ProgressType, ProgressFunc types\n- CrawlProject stub method (returns nil, nil)\n- crawl/crawl_test.go with type verification tests\n- make validate passes","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-11T17:44:52.252999-08:00","updated_at":"2025-12-11T18:12:56.469321-08:00","closed_at":"2025-12-11T18:12:56.469325-08:00","dependencies":[{"issue_id":"locdoc-fyy","depends_on_id":"locdoc-i7h","type":"parent-child","created_at":"2025-12-11T17:46:03.217457-08:00","created_by":"daemon"}]}
{"id":"locdoc-gc0","title":"Implement trafilatura/ package","description":"## Problem\n\nNeed content extraction to remove boilerplate from crawled pages.\n\n## Entrypoints\n\n- Create `trafilatura/` directory\n- Wrap go-trafilatura library API\n\n## Requirements\n\n- Accept raw HTML string\n- Return clean HTML + title (metadata)\n- Remove boilerplate (nav, footer, sidebar, ads)\n- Preserve main content structure\n- Use go-trafilatura library (not CLI)\n- Define interface in root package during implementation\n\n## Validation\n\n- [ ] Unit tests with HTML fixtures\n- [ ] Test against various doc site formats (Docusaurus, MkDocs, etc.)\n- [ ] `make validate` passes\n\n## References\n\n- docs/plans/2025-12-07-crawling-design.md\n- github.com/markusmobius/go-trafilatura examples/","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-07T22:01:24.742322-08:00","updated_at":"2025-12-09T12:04:36.268301-08:00","closed_at":"2025-12-09T12:04:36.268308-08:00","dependencies":[{"issue_id":"locdoc-gc0","depends_on_id":"locdoc-a4x","type":"blocks","created_at":"2025-12-07T22:01:57.250498-08:00","created_by":"daemon"},{"issue_id":"locdoc-gc0","depends_on_id":"locdoc-a3y","type":"parent-child","created_at":"2025-12-07T22:02:05.004233-08:00","created_by":"daemon"}]}
{"id":"locdoc-glt","title":"Path prefix filter not matching URLs without trailing slash","description":"## Resolution\n\n**Not a bug** - htmx.org's sitemap genuinely only contains 1 URL under `/docs/`:\n\n```\nPath breakdown (179 total URLs):\n49 /essays/\n36 /posts/\n36 /attributes/\n30 /examples/\n 9 /extensions/\n 7 /headers/\n 1 /docs/        ← Only this one!\n 1 /api/\n 1 /reference/\n```\n\nhtmx.org organizes documentation under `/attributes/`, `/api/`, `/reference/`, etc. - NOT under `/docs/`. The `/docs/` page is just a landing page.\n\n**The path prefix filtering works correctly.** To crawl htmx docs, use `https://htmx.org/` (no path filter) or target specific paths like `/attributes/`.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T07:55:47.487493-08:00","updated_at":"2025-12-10T07:57:30.738468-08:00","closed_at":"2025-12-10T07:57:30.738472-08:00"}
{"id":"locdoc-gvf","title":"Extract time parsing and query builder helpers in sqlite","description":"## Problem\nThe sqlite package has duplicated patterns:\n\n1. **Time parsing** (lines 66-74 in project.go and 82-86 in document.go):\n   ```go\n   project.CreatedAt, parseErr = time.Parse(time.RFC3339, createdAt)\n   if parseErr != nil {\n       return nil, fmt.Errorf(\"failed to parse created_at: %w\", parseErr)\n   }\n   ```\n\n2. **Query builder** for optional filters and pagination in both FindProjects and FindDocuments:\n   ```go\n   query.WriteString(\"WHERE 1=1\")\n   // repeated logic for optional filters\n   query.WriteString(\" ORDER BY ...\")\n   query.WriteString(\" LIMIT ?\")\n   query.WriteString(\" OFFSET ?\")\n   ```\n\n## Approach\n1. Create private parseRFC3339 helper function\n2. Consider query builder helper for pagination patterns\n3. Keep changes minimal - just extract duplication\n\n## Entrypoints\n- sqlite/project.go:66-74\n- sqlite/document.go:82-86, 111-116\n\n## Validation\n- [ ] Existing sqlite tests pass\n- [ ] No behavior changes\n- [ ] make validate passes","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-21T10:05:47.130086-08:00","updated_at":"2025-12-21T10:05:47.130086-08:00"}
{"id":"locdoc-h4h","title":"Add locdoc docs command","description":"## Problem\n\nNeed a way to inspect stored documents for a project.\n\n## Entrypoints\n\n- `/Users/filip/code/go/locdoc/cmd/locdoc/main.go`\n\n## Implementation\n\n1. Add to usage():\n```\ndocs \u003cname\u003e [--full]   List documents for a project (--full for content)\n```\n\n2. Add case to Run() switch\n\n3. Implement CmdDocs()\n\n### Default mode: `locdoc docs \u003cproject\u003e`\n\nSummary listing:\n```\nDocuments for inngest (42 total):\n\n  1. Getting Started\n     https://inngest.com/docs/getting-started\n  2. Functions\n     https://inngest.com/docs/functions\n```\n\n### Full mode: `locdoc docs \u003cproject\u003e --full`\n\nFull formatted content (same as what ask sends to Gemini):\n```\n## Document: Getting Started\n{full markdown content}\n\n## Document: Functions\n{full markdown content}\n```\n\nUses DocumentFormatter for full output.\n\n### Error messages\n\n- Project not found: `project \"foo\" not found. Use \"locdoc list\" to see available projects.`\n- No documents: `project \"foo\" has no documents. Run \"locdoc crawl foo\" first.`\n\n## Validation\n\n- [ ] `locdoc docs \u003cproject\u003e` shows summary listing\n- [ ] `locdoc docs \u003cproject\u003e --full` shows full content\n- [ ] Full output matches what ask command sends to LLM\n- [ ] Error messages are helpful\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-09T17:57:23.564894-08:00","updated_at":"2025-12-09T20:01:24.57742-08:00","closed_at":"2025-12-09T20:01:24.577423-08:00","dependencies":[{"issue_id":"locdoc-h4h","depends_on_id":"locdoc-4qp","type":"blocks","created_at":"2025-12-09T17:57:42.690932-08:00","created_by":"daemon"},{"issue_id":"locdoc-h4h","depends_on_id":"locdoc-9h9","type":"blocks","created_at":"2025-12-09T18:00:42.576965-08:00","created_by":"daemon"}]}
{"id":"locdoc-her","title":"Add browser recycling to prevent memory accumulation","description":"## Problem\nChrome accumulates memory over time (~0.5MB/s under load). Memory never returns to baseline even with proper page cleanup.\n\n## Solution\nImplement BrowserManager that recycles browser every 50-100 pages:\n\n```go\ntype BrowserManager struct {\n    browser   *rod.Browser\n    launcher  *launcher.Launcher\n    pageCount int64\n    maxPages  int64\n    mu        sync.Mutex\n}\n\nfunc (bm *BrowserManager) GetBrowser() *rod.Browser {\n    bm.mu.Lock()\n    defer bm.mu.Unlock()\n    \n    if atomic.LoadInt64(\u0026bm.pageCount) \u003e= bm.maxPages {\n        bm.recycleBrowser()\n    }\n    \n    atomic.AddInt64(\u0026bm.pageCount, 1)\n    return bm.browser\n}\n```\n\n## Entrypoints\n- rod/fetcher.go (or new rod/manager.go)\n\n## Validation\n- [ ] BrowserManager implemented\n- [ ] Browser recycled every N pages (configurable, default 75)\n- [ ] Tests verify recycling behavior\n- [ ] make validate passes\n\n## Research\nSee docs/go-rod-reliability.md","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-20T10:54:46.942447-08:00","updated_at":"2025-12-20T13:21:14.388778-08:00","closed_at":"2025-12-20T13:21:14.388781-08:00","dependencies":[{"issue_id":"locdoc-her","depends_on_id":"locdoc-y27","type":"blocks","created_at":"2025-12-20T10:54:51.316476-08:00","created_by":"daemon"}]}
{"id":"locdoc-hhk","title":"Add SQLite connection pool and busy timeout settings","description":"## Problem\n\nSQLite has connection pool and timeout settings that can prevent \"database is locked\" errors and improve robustness.\n\n## Entrypoints\n\n- `sqlite/sqlite.go` - `Open()` method\n\n## Suggested Settings\n\nFrom Ben Johnson's WTF:\n- `db.SetMaxOpenConns(1)` - SQLite only supports one writer anyway\n- `PRAGMA busy_timeout = 5000` - Wait 5s instead of failing immediately on lock\n\n## References\n\n- https://github.com/benbjohnson/wtf/blob/main/sqlite/sqlite.go\n\n## Validation\n\n- [ ] Settings applied in `Open()`\n- [ ] `make validate` passes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-08T13:20:05.81999-08:00","updated_at":"2025-12-09T19:37:05.779579-08:00","closed_at":"2025-12-09T19:37:05.779582-08:00"}
{"id":"locdoc-hw3","title":"Create CLAUDE.md with AI agent instructions","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-07T16:40:31.845746-08:00","updated_at":"2025-12-07T16:51:04.766651-08:00","closed_at":"2025-12-07T16:51:04.766654-08:00"}
{"id":"locdoc-i7h","title":"CLI refactor: Kong + crawl package extraction","description":"Refactor cmd/locdoc to use Kong CLI library and extract crawl orchestration to dedicated package. See docs/plans/2025-12-11-cli-refactor-design.md for full design.","acceptance_criteria":"- [ ] Kong handles all argument parsing\n- [ ] crawl/ package contains crawl orchestration logic\n- [ ] cmd/locdoc/ has one file per command\n- [ ] Test count reduced by ~80%\n- [ ] make validate passes\n- [ ] CLI interface unchanged (same commands, flags, output)","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-11T17:44:39.588662-08:00","updated_at":"2025-12-11T21:05:31.812934-08:00","closed_at":"2025-12-11T21:05:31.812937-08:00"}
{"id":"locdoc-iam","title":"Update Gemini asker tests for new prompt structure","description":"## Problem\nUpdate unit and integration tests to verify the new prompt structure works correctly.\n\n## Entrypoints\n- gemini/asker_test.go\n- gemini/asker_integration_test.go\n\n## Validation\n- [ ] Unit tests verify system instruction is set\n- [ ] Unit tests verify temperature is 0.4\n- [ ] Unit tests verify XML document structure\n- [ ] Integration test passes with real Gemini API\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T21:02:22.216806-08:00","updated_at":"2025-12-10T21:23:04.065207-08:00","closed_at":"2025-12-10T21:23:04.065219-08:00","dependencies":[{"issue_id":"locdoc-iam","depends_on_id":"locdoc-axo","type":"blocks","created_at":"2025-12-10T21:02:29.950252-08:00","created_by":"daemon"},{"issue_id":"locdoc-iam","depends_on_id":"locdoc-zml","type":"blocks","created_at":"2025-12-10T21:02:29.973858-08:00","created_by":"daemon"},{"issue_id":"locdoc-iam","depends_on_id":"locdoc-fx7","type":"blocks","created_at":"2025-12-10T21:02:29.997524-08:00","created_by":"daemon"}]}
{"id":"locdoc-il8","title":"Implement CmdAsk in CLI","description":"## Problem\n\nNeed CLI command to ask questions about project documentation.\n\n## Entrypoints\n\n- `cmd/locdoc/main.go`\n\n## Implementation\n\nAdd `CmdAsk` function that:\n1. Parses args: `locdoc ask \u003cproject\u003e \"question\"`\n2. Looks up project by name\n3. Fetches all documents for project\n4. Validates project has documents\n5. Calls `Asker.Ask()`\n6. Prints result to stdout\n\n## Error Messages\n\nProject not found:\n```\nproject \"foo\" not found. Use \"locdoc list\" to see available projects, or \"locdoc add \u003cname\u003e \u003curl\u003e\" to add one.\n```\n\nNo documents:\n```\nproject \"foo\" has no documents. Run \"locdoc crawl foo\" to fetch documentation first.\n```\n\nMissing API key:\n```\nGEMINI_API_KEY environment variable not set. Get an API key at https://aistudio.google.com/apikey\n```\n\n## Validation\n\n- [ ] `locdoc ask` with wrong args shows usage\n- [ ] Missing project shows helpful error\n- [ ] Empty project shows helpful error\n- [ ] Missing API key shows helpful error\n- [ ] Valid query returns answer\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-09T16:28:59.19651-08:00","updated_at":"2025-12-09T21:03:30.728479-08:00","closed_at":"2025-12-09T21:03:30.728482-08:00","dependencies":[{"issue_id":"locdoc-il8","depends_on_id":"locdoc-4nk","type":"blocks","created_at":"2025-12-09T16:29:04.632737-08:00","created_by":"daemon"},{"issue_id":"locdoc-il8","depends_on_id":"locdoc-n3w","type":"blocks","created_at":"2025-12-09T17:57:42.761146-08:00","created_by":"daemon"},{"issue_id":"locdoc-il8","depends_on_id":"locdoc-9h9","type":"blocks","created_at":"2025-12-09T18:00:42.600797-08:00","created_by":"daemon"}]}
{"id":"locdoc-ipo","title":"Centralize SQLite DB path resolution in main","description":"## Problem\nDB path resolution logic (checking LOCDOC_DB env var, falling back to ~/.locdoc/locdoc.db) should happen once in main and the resolved path injected to components that need it.\n\n## Entrypoints\n- `cmd/locdoc/main.go` - resolve DB path once at startup\n- Any component currently resolving the path independently\n\n## Validation\n- [ ] DB path resolved once in main\n- [ ] Path injected to sqlite.Open or equivalent\n- [ ] `make validate` passes","notes":"COMPLETED: Task was already implemented. Analysis confirmed:\n- defaultDBPath() in main.go:165-176 resolves LOCDOC_DB env var with fallback to ~/.locdoc/locdoc.db\n- NewMain() calls this once and stores in m.DBPath\n- Path injected to sqlite.NewDB(m.DBPath) at main.go:101\n- sqlite package has no knowledge of env vars or defaults\n\nAll validation criteria met.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-17T12:14:38.336874-08:00","updated_at":"2025-12-18T17:27:30.927238-08:00","closed_at":"2025-12-18T17:27:30.927241-08:00"}
{"id":"locdoc-jjg","title":"Clean up crawl tests: delete trivial tests and add helper","description":"## Problem\ncrawl_test.go has grown to 1500+ lines with accumulated cruft and repeated boilerplate.\n\n## Changes\n\n### Delete trivial tests (~60 lines)\n- `TestProgressType_Constants` - tests iota values, not useful\n- `TestResult_Fields` - tests struct has fields, compiler enforces\n- `TestProgressEvent_Fields` - same\n- `TestProgressFunc_Type` - tests callback is callable, trivial\n\n### Add test helper to reduce boilerplate\nEach test repeats ~50 lines of mock setup. Add:\n```go\nfunc newTestCrawler(opts ...func(*crawl.Crawler)) *crawl.Crawler\n```\n\n## Entrypoints\n- crawl/crawl_test.go\n\n## Validation\n- [ ] Trivial tests deleted\n- [ ] Helper added and used in existing tests\n- [ ] All tests still pass\n- [ ] make validate passes","notes":"COMPLETED: \n- Deleted 4 trivial tests (~60 lines): TestProgressType_Constants, TestResult_Fields, TestProgressEvent_Fields, TestProgressFunc_Type\n- Added newTestCrawler() helper with testMocks struct for customization\n- Refactored 12+ tests to use helper, reducing boilerplate\n- Net reduction: ~380 lines (1545 -\u003e 1168, about 25%)\n\nmake validate passes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-19T21:26:43.292176-08:00","updated_at":"2025-12-19T21:41:13.490754-08:00","closed_at":"2025-12-19T21:41:13.490756-08:00"}
{"id":"locdoc-k37","title":"Add rod.Fetcher lifecycle tests","description":"Missing tests for constructor and Close() behavior. Need to verify options pattern works, Close() is idempotent, and Fetch() after Close() returns appropriate error.","acceptance_criteria":"- [ ] Test NewFetcher with WithFetchTimeout option\n- [ ] Test Close() can be called multiple times safely\n- [ ] Test Fetch() after Close() returns error\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-15T20:50:08.49399-08:00","updated_at":"2025-12-17T12:05:55.728813-08:00","closed_at":"2025-12-17T12:05:55.728817-08:00"}
{"id":"locdoc-k4l","title":"Implement sqlite/ package","description":"## Problem\n\nNeed SQLite storage layer using pure Go driver (no CGO).\n\n## Entrypoints\n\n- Create `sqlite/` directory\n- Implement `ProjectService` interface from project.go\n- Implement `DocumentService` interface from document.go\n\n## Requirements\n\n- Use `ncruces/go-sqlite3` with `database/sql` driver\n- Auto-create schema on first run (migrations)\n- Store document content as Markdown text\n- Content hash (xxHash) for change detection\n- Compile-time interface verification: `var _ locdoc.ProjectService = (*ProjectService)(nil)`\n\n## Validation\n\n- [ ] Unit tests for CRUD operations\n- [ ] Integration test with real SQLite file\n- [ ] `make validate` passes\n- [ ] Builds with `CGO_ENABLED=0`\n\n## References\n\n- docs/plans/2025-12-07-crawling-design.md","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-07T22:00:47.279328-08:00","updated_at":"2025-12-08T12:37:13.040666-08:00","closed_at":"2025-12-08T12:37:13.04067-08:00","dependencies":[{"issue_id":"locdoc-k4l","depends_on_id":"locdoc-a3y","type":"parent-child","created_at":"2025-12-07T22:02:04.920979-08:00","created_by":"daemon"}]}
{"id":"locdoc-knx","title":"Create Makefile with validate target","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-07T16:40:31.93188-08:00","updated_at":"2025-12-07T16:52:33.128539-08:00","closed_at":"2025-12-07T16:52:33.128541-08:00"}
{"id":"locdoc-ksr","title":"Wire new dependencies in main","description":"Wire LinkSelectorRegistry and DomainLimiter in cmd/locdoc/main.go.\n\n## Entrypoints\n- Modify cmd/locdoc/main.go - create registry, limiter, inject into Crawler\n\n## Validation\n- CLI works end-to-end with recursive crawling\n- make validate passes","notes":"COMPLETED: All implementation and testing\n\n## Changes\n1. Wired LinkSelectorRegistry and DomainLimiter in cmd/locdoc/main.go:\n   - Added goquery package import\n   - Created Detector, GenericSelector, and Registry for link selection\n   - Created DomainLimiter with 1 RPS rate limit\n   - Registered all framework-specific selectors (Docusaurus, MkDocs, Sphinx, VuePress, GitBook, Nextra)\n   - Added separate tokenizerModel constant (gemini-2.5-flash) for token counting\n\n2. Fixed sitemap 404 handling in http/sitemap.go:\n   - processSitemap now returns empty slice (not error) when sitemap returns 404\n   - Enables recursive crawling fallback when robots.txt declares a nonexistent sitemap\n   - Added test: TestSitemapService_DiscoverURLs_SitemapDeclaredInRobotsBut404\n\n## Validation\n- make validate passes\n- Tested recursive crawling on TanStack Query (no sitemap) - successfully crawled 4 pages\n\n## Related Issues Created\n- locdoc-t6t: Support recursive crawling in preview mode (follow-up)\n- locdoc-okw: Update tokenizer to use gemini-3-flash when supported","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T16:08:11.236073-08:00","updated_at":"2025-12-19T07:45:12.444249-08:00","closed_at":"2025-12-19T07:45:12.444252-08:00","dependencies":[{"issue_id":"locdoc-ksr","depends_on_id":"locdoc-2yj","type":"parent-child","created_at":"2025-12-18T16:16:40.039095-08:00","created_by":"daemon"},{"issue_id":"locdoc-ksr","depends_on_id":"locdoc-5kd","type":"blocks","created_at":"2025-12-18T16:18:25.320194-08:00","created_by":"daemon"}]}
{"id":"locdoc-l3o","title":"Add mock implementations for new interfaces","description":"Add mock implementations to mock/ package for testing.\n\n## Entrypoints\n- Create/update mock/linkselector.go - LinkSelector, FrameworkDetector, LinkSelectorRegistry mocks\n- Create/update mock/frontier.go - URLFrontier, DomainLimiter mocks\n\n## Validation\n- All mocks have function fields matching interface methods\n- Compile-time interface verification\n- make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T16:08:10.615167-08:00","updated_at":"2025-12-18T16:36:16.077411-08:00","closed_at":"2025-12-18T16:36:16.077415-08:00","dependencies":[{"issue_id":"locdoc-l3o","depends_on_id":"locdoc-2yj","type":"parent-child","created_at":"2025-12-18T16:16:39.921225-08:00","created_by":"daemon"},{"issue_id":"locdoc-l3o","depends_on_id":"locdoc-phn","type":"blocks","created_at":"2025-12-18T16:18:20.086323-08:00","created_by":"daemon"}]}
{"id":"locdoc-l52","title":"Improve crawl command output with summary statistics","description":"## Problem\n\nAfter making crawl concurrent, the output now dumps as a wall of text at the end instead of streaming progress. The verbose per-URL output isn't useful when it all appears at once.\n\n## Design\n\nSee [docs/plans/2025-12-10-crawl-summary-token-counting.md](../docs/plans/2025-12-10-crawl-summary-token-counting.md)\n\n**Summary:**\n- Add `TokenCounter` interface to root package\n- Implement in `gemini/` package using `google.golang.org/genai/tokenizer`\n- Model const (`gemini-2.5-flash`) in main, passed to tokenizer\n- Count tokens during crawl results loop, accumulate totals\n- Replace per-URL output with summary: `Saved 179 pages (2.3 MB, ~580k tokens)`\n\n## Entrypoints\n\n- `locdoc.go` - Add TokenCounter interface\n- `gemini/token.go` - New implementation\n- `mock/token.go` - New mock\n- `cmd/locdoc/main.go` - Wire tokenizer, update crawlProject output\n\n## Validation\n\n- [ ] Crawl shows summary instead of per-URL output\n- [ ] Summary includes page count, size, and token count\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T08:00:39.134594-08:00","updated_at":"2025-12-10T09:26:21.02203-08:00","closed_at":"2025-12-10T09:26:21.022032-08:00"}
{"id":"locdoc-lf5","title":"Implement URL frontier with priority queue","description":"Create in-memory URLFrontier with priority queue and Bloom filter deduplication.\n\n## Entrypoints\n- Create crawl/frontier.go - Frontier struct implementing locdoc.URLFrontier\n- Use container/heap for priority queue\n- Use bloom/ package for deduplication\n- Create crawl/frontier_test.go\n\n## Validation\n- Pop() returns highest priority URL first\n- Push() returns false for already-seen URLs\n- Memory-efficient for 10k+ URLs\n- make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T16:08:10.234413-08:00","updated_at":"2025-12-18T17:53:56.303628-08:00","closed_at":"2025-12-18T17:53:56.303631-08:00","dependencies":[{"issue_id":"locdoc-lf5","depends_on_id":"locdoc-2yj","type":"parent-child","created_at":"2025-12-18T16:16:39.862066-08:00","created_by":"daemon"},{"issue_id":"locdoc-lf5","depends_on_id":"locdoc-8pb","type":"blocks","created_at":"2025-12-18T16:18:22.558785-08:00","created_by":"daemon"}]}
{"id":"locdoc-lif","title":"Deduplicate URLs that differ only by fragment","description":"Recursive crawling currently treats URLs with different fragments as separate pages (e.g., /overview and /overview#motivation are saved as separate documents).\n\n## Problem\nWhen crawling TanStack Query docs, the same page was saved 4 times:\n- https://tanstack.com/query/latest/docs/framework/react/overview\n- https://tanstack.com/query/latest/docs/framework/react/overview#motivation\n- https://tanstack.com/query/latest/docs/framework/react/overview#you-talked-me-into-it-so-what-now\n- https://tanstack.com/query/latest/docs/framework/react/overview#enough-talk-show-me-some-code-already\n\n## Entrypoints\n- crawl/crawl.go - recursiveCrawl method, URL normalization before frontier.Push\n- Possibly goquery/selector_*.go - ExtractLinks could strip fragments\n\n## Approach\nStrip URL fragments before:\n1. Adding to frontier (dedup check)\n2. Saving documents\n\n## Validation\n- Crawling a page with anchor links saves only one document per unique path\n- make validate passes","notes":"COMPLETED: URL fragment deduplication\n\nIMPLEMENTATION:\n1. Frontier (crawl/frontier.go):\n   - Modified Push() to strip URL fragments before deduplication\n   - Modified Seen() to strip fragments for consistency\n\n2. Link Extraction (goquery/selector_base.go):\n   - Modified resolveURL() to strip fragments and filter self-referential links\n   - This applies to ALL framework-specific selectors (MkDocs, Docusaurus, Sphinx, etc.)\n\nKEY BEHAVIORS:\n- URLs with same base path but different fragments are treated as duplicates\n- Stored URLs have fragments stripped\n- Anchor-only links (#section) are filtered as self-referential\n- Query parameters are preserved\n\nTESTS ADDED:\n- Frontier: fragment deduplication, edge cases (empty fragment, multiple #, special chars)\n- goquery: fragment stripping, anchor-only link filtering, deduplication by fragment\n- Updated framework-specific selector tests for new behavior\n\nVALIDATION: make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T20:35:21.690354-08:00","updated_at":"2025-12-19T16:50:33.317149-08:00","closed_at":"2025-12-19T16:50:33.317152-08:00","dependencies":[{"issue_id":"locdoc-lif","depends_on_id":"locdoc-ksr","type":"blocks","created_at":"2025-12-18T20:35:26.498521-08:00","created_by":"daemon"}]}
{"id":"locdoc-m6o","title":"Fix rod.Fetcher launcher resource leak","description":"The launcher process is created but not stored in NewFetcher, making proper cleanup impossible in Close(). The launcher should be stored in the Fetcher struct and killed during Close().","acceptance_criteria":"- [ ] Launcher stored in Fetcher struct\n- [ ] Close() kills launcher process after closing browser\n- [ ] make validate passes","notes":"COMPLETED: \n- Added launcher field to Fetcher struct\n- Store launcher instance in NewFetcher\n- Added LauncherPID() method for testing\n- Updated Close() to kill launcher after closing browser\n- Added integration test verifying launcher cleanup\n\nVALIDATION: make validate passes","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-15T20:50:08.384931-08:00","updated_at":"2025-12-15T20:56:08.446975-08:00","closed_at":"2025-12-15T20:56:08.446978-08:00"}
{"id":"locdoc-mdd","title":"Implement htmltomd/ package","description":"## Problem\n\nNeed HTML to Markdown conversion for storage.\n\n## Entrypoints\n\n- Create `htmltomarkdown/` directory\n- Wrap html-to-markdown library\n\n## Requirements\n\n- Accept clean HTML string\n- Return Markdown string\n- Preserve code blocks with language hints\n- Preserve tables, links, headings\n- Use `JohannesKaufmann/html-to-markdown` library\n- Define interface in root package during implementation\n\n## Validation\n\n- [ ] Unit tests with HTML fixtures\n- [ ] Test code block preservation\n- [ ] Test table conversion\n- [ ] `make validate` passes\n\n## References\n\n- docs/plans/2025-12-07-crawling-design.md\n- github.com/JohannesKaufmann/html-to-markdown","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-07T22:01:24.868426-08:00","updated_at":"2025-12-09T12:24:43.88477-08:00","closed_at":"2025-12-09T12:24:43.884774-08:00","dependencies":[{"issue_id":"locdoc-mdd","depends_on_id":"locdoc-gc0","type":"blocks","created_at":"2025-12-07T22:01:57.281633-08:00","created_by":"daemon"},{"issue_id":"locdoc-mdd","depends_on_id":"locdoc-a3y","type":"parent-child","created_at":"2025-12-07T22:02:05.032944-08:00","created_by":"daemon"}]}
{"id":"locdoc-mp2","title":"Improve ask prompt for retrieval over problem-solving","description":"## Problem\nThe current Gemini prompt encourages problem-solving rather than information retrieval. When asked about troubleshooting, the model tries to solve the problem rather than surfacing relevant documentation passages.\n\n## Research\nSee docs/prompt-for-retrieval-not-solving.md for detailed findings. Key techniques:\n- Tagged context with explicit anchors (98% hallucination reduction)\n- Evidence-first response structure (quotes before synthesis)\n- Behavioral constraints over persona (\"documentation navigator\" not \"helpful assistant\")\n- Epistemic markers (\"The documentation states...\" vs \"I recommend...\")\n- Instruction hierarchy to prevent override\n\n## Changes\nUpdate gemini/asker.go:\n\n1. **BuildConfig()** - Update system instruction:\n   - Change persona from \"helpful assistant\" to \"documentation navigator\"\n   - Add explicit constraints: \"do NOT provide solutions, code, or recommendations\"\n   - Add instruction hierarchy with refusal pattern\n\n2. **BuildUserPrompt()** - Update structure:\n   - Add `[DOC: title]` tags for each document\n   - Require evidence-first format: quotes → synthesis → gaps\n   - Require epistemic markers\n   - Update citation format to use URLs with anchors\n\n## Entrypoints\n- gemini/asker.go:64-103 (BuildConfig and BuildUserPrompt)\n- docs/prompt-for-retrieval-not-solving.md (reference)\n\n## Validation\n- [ ] Test with problem-solving query (e.g., \"how do I fix X error?\")\n- [ ] Response quotes docs before synthesizing\n- [ ] Response says \"not covered\" instead of inferring when info missing\n- [ ] Citations include URLs (with anchors when available)\n- [ ] make validate passes","notes":"COMPLETED: Updated BuildConfig and BuildUserPrompt for retrieval behavior\n- Changed persona from 'helpful assistant' to 'documentation navigator'\n- Added explicit constraints (do NOT provide solutions, do NOT generate novel content)\n- Added instruction hierarchy with refusal pattern\n- Updated user prompt with evidence-first structure (RELEVANT DOCUMENTATION → ANSWER BASED ON ABOVE → NOT COVERED)\n- Updated citation format to require URL#anchor when citing sections\n\nIN_PROGRESS: Self-review before finishing","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T11:38:15.365906-08:00","updated_at":"2025-12-21T12:13:05.077015-08:00","closed_at":"2025-12-21T12:13:05.077018-08:00"}
{"id":"locdoc-mtp","title":"Use fast HTTP fetcher with rod fallback for preview mode","description":"## Problem\nPreview mode is very slow because it uses rod (headless Chrome) for every page fetch during recursive discovery. This is overkill for most documentation sites that serve static or server-rendered HTML.\n\n## Proposed Solution\n1. Try fast HTTP client first (like `net/http`)\n2. If page appears to need JavaScript (empty body, SPA markers), fall back to rod\n3. Or: add a `--fast` flag that skips rod entirely for preview\n\n## Benefits\n- 10-100x faster for static sites\n- Still works for SPAs when needed\n- Better UX for preview mode\n\n## Considerations\n- Need heuristic to detect when rod is needed\n- Some sites require JavaScript for navigation links (like TanStack)\n- Could make fast mode the default for preview, rod for full crawl\n\n## Validation\n- Preview mode completes in seconds, not minutes\n- Still discovers links on JavaScript-heavy sites","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-19T20:03:17.334912-08:00","updated_at":"2025-12-19T20:04:21.464569-08:00","closed_at":"2025-12-19T20:04:21.464569-08:00"}
{"id":"locdoc-mtz","title":"Implement http/ package","description":"## Problem\n\nNeed sitemap discovery and parsing for URL extraction.\n\n## Entrypoints\n\n- Create `http/` directory\n- Port sitemap logic from go-trafilatura cmd/\n\n## Requirements\n\n- Discover sitemap URL from robots.txt (`Sitemap:` directive)\n- Fall back to `/sitemap.xml` if not found\n- Parse sitemap XML using `beevik/etree`\n- Handle sitemap indexes recursively\n- Filter URLs by pattern (include/exclude regex)\n- Return list of discovered URLs\n- Respect context cancellation\n- Define interface in root package during implementation\n\n## Validation\n\n- [ ] Unit tests with sitemap XML fixtures\n- [ ] Integration test against real site (e.g., go.dev)\n- [ ] `make validate` passes\n\n## References\n\n- docs/plans/2025-12-07-crawling-design.md\n- github.com/markusmobius/go-trafilatura/cmd/go-trafilatura/sitemap.go","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-07T22:01:24.476838-08:00","updated_at":"2025-12-08T13:38:49.035033-08:00","closed_at":"2025-12-08T13:38:49.035037-08:00","dependencies":[{"issue_id":"locdoc-mtz","depends_on_id":"locdoc-a3y","type":"parent-child","created_at":"2025-12-07T22:02:04.951025-08:00","created_by":"daemon"}]}
{"id":"locdoc-n3w","title":"Capture sitemap position during crawl","description":"## Problem\n\ncrawlProject() has the loop index but does not store it as document position.\n\n## Entrypoints\n\n- `/Users/filip/code/go/locdoc/cmd/locdoc/main.go` - crawlProject()\n\n## Implementation\n\n1. Set Position when creating document:\n```go\ndoc := \u0026locdoc.Document{\n    // ... existing fields ...\n    Position: i,\n}\n```\n\n2. Include position when updating:\n```go\nposition := i\ndocuments.UpdateDocument(ctx, existing.ID, locdoc.DocumentUpdate{\n    // ... existing fields ...\n    Position: \u0026position,\n})\n```\n\n3. Update position even when content unchanged:\n```go\nif existing != nil \u0026\u0026 existing.ContentHash == hash {\n    if existing.Position != i {\n        position := i\n        documents.UpdateDocument(ctx, existing.ID, locdoc.DocumentUpdate{\n            Position: \u0026position,\n        })\n        fmt.Fprintln(stdout, \"    position updated\")\n    } else {\n        fmt.Fprintln(stdout, \"    unchanged\")\n    }\n    continue\n}\n```\n\n## Validation\n\n- [ ] New documents get correct position\n- [ ] Updated documents get correct position\n- [ ] Position updates when content unchanged but position changed\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-09T17:57:16.281017-08:00","updated_at":"2025-12-09T19:14:08.609013-08:00","closed_at":"2025-12-09T19:14:08.609016-08:00","dependencies":[{"issue_id":"locdoc-n3w","depends_on_id":"locdoc-4qp","type":"blocks","created_at":"2025-12-09T17:57:42.619239-08:00","created_by":"daemon"}]}
{"id":"locdoc-ndt","title":"Flush stdout after progress updates","description":"## Problem\nProgress line during crawl uses carriage return (\\\\r) for in-place updates, but Go's fmt.Fprintf to os.Stdout is buffered. Output may sit in buffer and not render until a newline forces a flush, making progress invisible.\n\n## Entrypoints\n- `cmd/locdoc/main.go` - `crawlProject()` around line 510\n\n## Validation\n- [ ] Progress line visible during crawl (test with slow network or large site)\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T18:43:14.85788-08:00","updated_at":"2025-12-10T18:44:44.484418-08:00","closed_at":"2025-12-10T18:44:44.484421-08:00"}
{"id":"locdoc-nle","title":"Add Kong dependency and create CLI struct skeleton","description":"Add Kong to go.mod. Create basic CLI struct with all 5 command structs (AddCmd, ListCmd, DeleteCmd, DocsCmd, AskCmd) with correct struct tags. Wire Kong parser in Main.Run(). Commands can be empty stubs that return nil.","acceptance_criteria":"- [ ] go get github.com/alecthomas/kong\n- [ ] CLI struct with 5 command structs defined\n- [ ] Kong parser created with Writers(stdout, stderr)\n- [ ] locdoc --help shows all commands\n- [ ] make validate passes","notes":"COMPLETED: Kong CLI skeleton\n- Added github.com/alecthomas/kong dependency\n- Created CLI struct with AddCmd, ListCmd, DeleteCmd, DocsCmd, AskCmd\n- All command structs have proper Kong struct tags\n- All commands have stub Run() methods returning nil\n- Kong parser wired in Main.Run() with Writers(stdout, stderr)\n- --help now uses Kong formatting\n- All tests passing, make validate passes","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-11T17:44:46.090614-08:00","updated_at":"2025-12-11T17:59:39.280092-08:00","closed_at":"2025-12-11T17:59:39.280094-08:00","dependencies":[{"issue_id":"locdoc-nle","depends_on_id":"locdoc-i7h","type":"parent-child","created_at":"2025-12-11T17:46:03.193323-08:00","created_by":"daemon"}]}
{"id":"locdoc-nnd","title":"Create .claude/ commands","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-07T16:40:32.484792-08:00","updated_at":"2025-12-07T20:31:12.324995-08:00","closed_at":"2025-12-07T20:31:12.324998-08:00"}
{"id":"locdoc-nwx","title":"Implement base link selector with goquery","description":"Create goquery/ package with shared baseSelector logic for extracting prioritized links from HTML.\n\n## Entrypoints\n- Create goquery/selector_base.go - baseSelector struct with ExtractLinks() using CSS selectors\n- Handle URL resolution, deduplication, priority scoring\n- Create goquery/selector_base_test.go\n\n## Validation\n- Extracts links from nav/aside/main/footer with correct priorities\n- Resolves relative URLs correctly\n- Filters external links\n- make validate passes","notes":"COMPLETED: Implementation of BaseSelector in goquery/ package\n- Implements locdoc.LinkSelector interface\n- Extracts links from nav (PriorityNavigation), aside (PriorityTOC), main/article (PriorityContent), footer (PriorityFooter)\n- Resolves relative URLs correctly\n- Filters external links (different host)\n- Deduplicates links keeping highest priority\n- All tests pass with make validate","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T16:07:52.131301-08:00","updated_at":"2025-12-18T16:51:02.436642-08:00","closed_at":"2025-12-18T16:51:02.436644-08:00","dependencies":[{"issue_id":"locdoc-nwx","depends_on_id":"locdoc-2yj","type":"parent-child","created_at":"2025-12-18T16:16:39.544736-08:00","created_by":"daemon"},{"issue_id":"locdoc-nwx","depends_on_id":"locdoc-phn","type":"blocks","created_at":"2025-12-18T16:18:19.964609-08:00","created_by":"daemon"}]}
{"id":"locdoc-o5n","title":"Implement DocsCmd with Kong","description":"Create cmd/locdoc/docs.go with DocsCmd struct and Run method. Lists documents for a project. --full flag shows full content.","acceptance_criteria":"- [ ] cmd/locdoc/docs.go exists\n- [ ] DocsCmd struct with Name arg and Full flag\n- [ ] Run method shows document list (title, URL)\n- [ ] --full shows formatted content via locdoc.FormatDocuments\n- [ ] Error if project not found or has no documents\n- [ ] cmd/locdoc/docs_test.go with integration tests\n- [ ] make validate passes","notes":"COMPLETED: All acceptance criteria met\n- cmd/locdoc/docs.go created with DocsCmd.Run method\n- DocsCmd struct with Name arg and Full flag (already in cli.go)\n- Run method shows document list (title, URL) in summary mode\n- --full flag shows formatted content via locdoc.FormatDocuments\n- Error handling for project not found and no documents\n- cmd/locdoc/docs_test.go with 7 integration tests\n- make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T17:45:25.265976-08:00","updated_at":"2025-12-11T20:09:33.509061-08:00","closed_at":"2025-12-11T20:09:33.509064-08:00","dependencies":[{"issue_id":"locdoc-o5n","depends_on_id":"locdoc-i7h","type":"parent-child","created_at":"2025-12-11T17:46:03.357382-08:00","created_by":"daemon"},{"issue_id":"locdoc-o5n","depends_on_id":"locdoc-8cu","type":"blocks","created_at":"2025-12-11T17:48:04.018704-08:00","created_by":"daemon"}]}
{"id":"locdoc-o73","title":"Emit ProgressFailed when CreateDocument fails in recursive crawl","description":"## Problem\nIn `recursiveCrawl`, when `CreateDocument` fails, we increment `result.Failed` but don't emit a `ProgressFailed` event. Users don't see feedback about which URLs failed to save.\n\nOther failures (fetch, extract, convert) all emit `ProgressFailed` events.\n\n## Entrypoints\n- crawl/crawl.go:420-424 - CreateDocument failure handling in recursiveCrawl\n\n## Validation\n- [ ] CreateDocument failure emits ProgressFailed with URL and error\n- [ ] User sees 'skip' message for save failures (same as fetch failures)\n- [ ] make validate passes","notes":"COMPLETED: Added test coverage for ProgressFailed emission on CreateDocument failure\nKEY_FINDING: The implementation was already in place (added in commit 5507b8b when recursive crawl was refactored to use concurrent workers). The issue was created before that refactor.\nTEST ADDED: 'recursive crawl emits ProgressFailed when CreateDocument fails' in crawl/crawl_test.go\nVALIDATION: make validate passes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-19T18:29:20.474636-08:00","updated_at":"2025-12-19T21:56:15.930014-08:00","closed_at":"2025-12-19T21:56:15.930018-08:00"}
{"id":"locdoc-oj9","title":"Add rod.Fetcher concurrent usage test","description":"Line 21 claims Fetcher is safe for concurrent use, but there are no tests verifying this. Add a test that calls Fetch() from multiple goroutines concurrently to validate thread-safety under race detector.","acceptance_criteria":"- [ ] Test concurrent Fetch() calls from multiple goroutines\n- [ ] Test passes with -race flag\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-15T20:50:08.601455-08:00","updated_at":"2025-12-17T20:19:44.221592-08:00","closed_at":"2025-12-17T20:19:44.221595-08:00"}
{"id":"locdoc-okw","title":"Update tokenizer to use gemini-3-flash when supported","description":"The local tokenizer currently doesn't support gemini-3-flash-preview, so we're using gemini-2.5-flash as a workaround.\n\n## Problem\ngemini.NewTokenCounter('gemini-3-flash-preview') fails with 'model not supported'.\n\n## Current Workaround\nUsing separate constants in cmd/locdoc/main.go:\n- defaultModel = gemini-3-flash-preview (for Asker)\n- tokenizerModel = gemini-2.5-flash (for TokenCounter)\n\n## Validation\nWhen gemini-3-flash is supported by google.golang.org/genai/tokenizer:\n- Update tokenizerModel to match defaultModel\n- Remove the separate constant if possible\n- make validate passes","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-18T20:31:19.431474-08:00","updated_at":"2025-12-18T20:31:19.431474-08:00"}
{"id":"locdoc-ot9","title":"Remove crawl command","description":"Remove the standalone crawl command now that add handles crawling.\n\n## Changes\n- Remove CmdCrawl function\n- Remove crawl case from command dispatch\n- Update help text\n- Update any docs referencing crawl\n\n## Entrypoints\n- cmd/locdoc/main.go\n\n## Validation\n- [ ] crawl command removed\n- [ ] Help text updated\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T11:06:35.912545-08:00","updated_at":"2025-12-10T14:28:15.605143-08:00","closed_at":"2025-12-10T14:28:15.605146-08:00","dependencies":[{"issue_id":"locdoc-ot9","depends_on_id":"locdoc-co5","type":"blocks","created_at":"2025-12-10T11:06:44.640437-08:00","created_by":"daemon"},{"issue_id":"locdoc-ot9","depends_on_id":"locdoc-9f6","type":"blocks","created_at":"2025-12-10T11:06:44.70731-08:00","created_by":"daemon"}]}
{"id":"locdoc-phn","title":"Define domain types for link selection and frontier","description":"Add domain types and interfaces for link selection, framework detection, and URL frontier to the root package.\n\n## Entrypoints\n- Create linkselector.go - LinkPriority, DiscoveredLink, Framework, LinkSelector, FrameworkDetector, LinkSelectorRegistry\n- Create frontier.go - URLFrontier, DomainLimiter interfaces\n\n## Validation\n- Types compile with no external dependencies in root package\n- make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T16:07:51.388967-08:00","updated_at":"2025-12-18T16:23:26.718308-08:00","closed_at":"2025-12-18T16:23:26.71831-08:00","dependencies":[{"issue_id":"locdoc-phn","depends_on_id":"locdoc-2yj","type":"parent-child","created_at":"2025-12-18T16:16:39.41483-08:00","created_by":"daemon"}]}
{"id":"locdoc-pkc","title":"Extract shared link extraction logic in goquery","description":"## Problem\nThe ExtractLinks method is duplicated ~90 lines across 8 selector files in goquery/:\n- selector_base.go\n- selector_generic.go  \n- selector_docusaurus.go\n- selector_mkdocs.go\n- selector_sphinx.go\n- selector_gitbook.go\n- selector_vuepress.go\n- selector_nextra.go\n\nThis violates DRY and creates maintenance burden. ~600 lines could become ~100.\n\n## Approach\n1. Create a SelectorConfig struct with Selector, Priority, Source fields\n2. Extract shared extraction logic into a helper function\n3. Each framework defines only its selector configurations\n4. Helper handles all boilerplate (URL resolution, deduplication, validation)\n\n## Entrypoints\n- goquery/selector_base.go (lines 38-102 show the pattern)\n- goquery/selector_docusaurus.go (lines 35-108 duplicate it)\n\n## Validation\n- [ ] All existing selector tests pass\n- [ ] No behavior changes\n- [ ] make validate passes","status":"in_progress","priority":2,"issue_type":"task","created_at":"2025-12-21T10:05:45.058996-08:00","updated_at":"2025-12-21T12:44:31.51197-08:00"}
{"id":"locdoc-sbw","title":"Split Crawler into preview and full crawl variants","description":"## Problem\nThe crawl.Crawler struct has 9 fields, but 3 are only set for non-preview mode:\n- Converter\n- Documents  \n- TokenCounter\n\nThese are checked with nil guards throughout the code, creating unclear contracts and potential runtime panics if misused.\n\nAdditionally, two distinct execution paths (sitemap vs recursive crawling) are tangled together with different field requirements.\n\n## Approach\nOptions to consider:\n1. Split into CrawlerCore + FullCrawler (embeds core, adds required fields)\n2. Add validation at construction time with clear error messages\n3. Create separate DiscoveryOptions vs CrawlOptions structs\n\n## Entrypoints\n- crawl/crawl.go (Crawler struct definition)\n- cmd/locdoc/main.go:155-176 (conditional field assignment)\n\n## Validation\n- [ ] Existing crawler tests pass\n- [ ] Preview mode works without Converter/Documents/TokenCounter\n- [ ] Full crawl fails fast if required fields missing\n- [ ] make validate passes","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-21T10:05:46.117256-08:00","updated_at":"2025-12-21T10:05:46.117256-08:00"}
{"id":"locdoc-sl6","title":"Add retry logic with exponential backoff for fetch failures","description":"## Problem\nTransient fetch failures (timeouts, network issues) cause permanent URL skips with no recovery.\n\n## Entrypoints\n- `cmd/locdoc/main.go` - `processURL()` function\n\n## Validation\n- [ ] Fetch failures retry up to 3 times with 1s→2s→4s backoff\n- [ ] Only fetch stage retries (not extract/convert)\n- [ ] Retry attempts logged or visible in output\n- [ ] `make validate` passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T15:41:07.529599-08:00","updated_at":"2025-12-10T17:28:10.234925-08:00","closed_at":"2025-12-10T17:28:10.234928-08:00","dependencies":[{"issue_id":"locdoc-sl6","depends_on_id":"locdoc-57u","type":"blocks","created_at":"2025-12-10T15:41:22.655826-08:00","created_by":"daemon"}]}
{"id":"locdoc-smz","title":"Remove unused URLFrontier mock","description":"## Problem\nThe mock.URLFrontier in mock/frontier.go (lines 9-33) is never used in any test file. The DomainLimiter mock in the same file IS used.\n\n## Approach\n1. Remove URLFrontier mock (lines 9-33)\n2. Keep DomainLimiter mock (lines 35-44)\n3. Update doc.go if needed\n\n## Entrypoints\n- mock/frontier.go\n\n## Validation\n- [ ] All tests pass\n- [ ] make validate passes","status":"closed","priority":4,"issue_type":"task","created_at":"2025-12-21T10:05:49.167594-08:00","updated_at":"2025-12-21T12:38:16.733251-08:00","closed_at":"2025-12-21T12:38:16.733255-08:00"}
{"id":"locdoc-t0c","title":"Implement CrawlProject in crawl package","description":"Move crawlProject() and processURL() logic from main.go to crawl/crawl.go as Crawler.CrawlProject method. Adapt to use ProgressFunc callback instead of direct stdout/stderr writes. Include helper functions (truncateURL, formatBytes, formatTokens, computeHash).","acceptance_criteria":"- [ ] Crawler.CrawlProject fully implemented\n- [ ] Uses ProgressFunc for all progress reporting\n- [ ] Helper functions moved to crawl package\n- [ ] crawl/crawl_test.go with tests using mock services\n- [ ] make validate passes","notes":"COMPLETED: All acceptance criteria met\n- Crawler.CrawlProject fully implemented with concurrent URL processing\n- Uses ProgressFunc for all progress reporting (Started, Completed, Failed, Finished events)\n- Helper functions (TruncateURL, FormatBytes, FormatTokens, ComputeHash) moved to crawl package and exported\n- crawl/crawl_test.go contains tests using mock services covering: empty URL list, single URL crawl, fetch failures, progress callbacks\n- main.go updated to use crawl package helper functions\n- make validate passes","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-11T17:45:03.653418-08:00","updated_at":"2025-12-11T19:13:40.619973-08:00","closed_at":"2025-12-11T19:13:40.619976-08:00","dependencies":[{"issue_id":"locdoc-t0c","depends_on_id":"locdoc-i7h","type":"parent-child","created_at":"2025-12-11T17:46:03.264703-08:00","created_by":"daemon"},{"issue_id":"locdoc-t0c","depends_on_id":"locdoc-fyy","type":"blocks","created_at":"2025-12-11T17:48:03.892026-08:00","created_by":"daemon"},{"issue_id":"locdoc-t0c","depends_on_id":"locdoc-a3x","type":"blocks","created_at":"2025-12-11T17:48:03.918979-08:00","created_by":"daemon"}]}
{"id":"locdoc-t6t","title":"Support recursive crawling in preview mode","description":"Preview mode should fall back to recursive link discovery when sitemap is unavailable, instead of returning an error.\n\n## Problem\nCurrently `locdoc add --preview` only uses sitemap discovery. When a site has no sitemap (e.g., https://tanstack.com/query/latest/docs/), it errors with 'HTTP 404 for sitemap.xml' instead of falling back to recursive crawling.\n\n## Entrypoints\n- cmd/locdoc/add.go - AddCmd.Run preview path (lines 33-43)\n- cmd/locdoc/main.go - may need to create LinkSelectors/RateLimiter for preview mode too\n\n## Approach\nEither:\n1. Create Crawler dependencies in preview mode and add a 'preview-only' crawl method\n2. Or extract link discovery logic that can be shared between preview and full crawl\n\n## Validation\n- `locdoc add --preview testdocs https://tanstack.com/query/latest/docs/` returns URLs instead of error\n- make validate passes","notes":"COMPLETED: Implemented recursive URL discovery for preview mode\n\nChanges:\n- Added DiscoverURLs function to crawl package\n- Added Fetcher, LinkSelectors, RateLimiter fields to Dependencies\n- Updated AddCmd.Run to fall back to recursive discovery when sitemap empty\n- Updated main.go to wire discovery dependencies for preview mode\n\nCode review addressed:\n- Added unit tests for DiscoverURLs (5 test cases covering recursion, scope, filtering, error handling, cancellation)\n- Added safety limit documentation to DiscoverURLs doc comment\n- Did NOT refactor duplication with recursiveCrawl - intentional, functions have different responsibilities\n\nAll tests pass, make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T20:28:24.038502-08:00","updated_at":"2025-12-19T19:49:50.896162-08:00","closed_at":"2025-12-19T19:49:50.896165-08:00","dependencies":[{"issue_id":"locdoc-t6t","depends_on_id":"locdoc-ksr","type":"blocks","created_at":"2025-12-18T20:28:34.623629-08:00","created_by":"daemon"}]}
{"id":"locdoc-tma","title":"Split crawl package into smaller files","description":"## Problem\ncrawl.go (764 lines) and crawl_test.go (1227 lines) are large files that could be split for better organization.\n\n## Approach\nSplit into focused files:\n\n**Implementation:**\n- crawl.go - Crawler struct, CrawlProject, processURL\n- walk.go - walkFrontier, walkProcessor, walkResultHandler, processRecursiveURL, processRecursiveResult\n- discover.go - DiscoverURLs function\n- format.go - TruncateURL, FormatBytes, FormatTokens, ComputeHash\n\n**Tests (matching):**\n- crawl_test.go - TestCrawler_CrawlProject, newTestCrawler helper\n- walk_test.go - TestRecursiveCrawl_Concurrency\n- discover_test.go - TestDiscoverURLs\n- format_test.go - TestTruncateURL, TestFormatBytes, TestFormatTokens, TestComputeHash\n\n## Entrypoints\n- crawl/crawl.go\n- crawl/crawl_test.go\n\n## Validation\n- [ ] Files split as described\n- [ ] All tests pass\n- [ ] make validate passes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-19T22:37:22.808723-08:00","updated_at":"2025-12-19T23:05:36.393904-08:00","closed_at":"2025-12-19T23:05:36.393907-08:00"}
{"id":"locdoc-tsu","title":"Add filter column to projects table","description":"Add filter TEXT column to projects table schema. Update Project struct and SQLite implementation to read/write filter field.\n\n## Entrypoints\n- locdoc.go (Project struct)\n- sqlite/project.go\n- sqlite/schema.go\n\n## Validation\n- [ ] Project struct has Filter field\n- [ ] CreateProject/FindProjects handle filter\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T11:06:02.629597-08:00","updated_at":"2025-12-10T12:00:27.221029-08:00","closed_at":"2025-12-10T12:00:27.221034-08:00"}
{"id":"locdoc-v6q","title":"Strengthen rod integration test assertions","description":"Current integration tests only check for string presence (e.g., 'htmx' appears somewhere). This is weak - could pass even without JS rendering. Tests should verify actual rendered DOM structure.","acceptance_criteria":"- [ ] Tests verify HTML document structure (opening/closing tags)\n- [ ] Tests check for specific rendered content that requires JS execution\n- [ ] make validate passes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-15T20:50:08.717091-08:00","updated_at":"2025-12-17T19:59:31.614169-08:00","closed_at":"2025-12-17T19:59:31.614172-08:00"}
{"id":"locdoc-wfk","title":"Implement cmd/locdoc/ CLI","description":"## Problem\n\nNeed CLI to wire all packages together and provide user interface.\n\n## Entrypoints\n\n- Create or update `cmd/locdoc/` directory\n- Wire all implementation packages\n\n## Requirements\n\n- `locdoc add \u003cname\u003e \u003curl\u003e` - register project with sitemap URL\n- `locdoc list` - show registered projects\n- `locdoc crawl [name]` - run full pipeline for all/one project\n- Wire: http/ → rod/ → trafilatura/ → htmltomd/ → sqlite/\n- Exit codes for success/failure\n- Progress output during crawl\n\n## Validation\n\n- [ ] End-to-end test: add project, crawl, verify documents stored\n- [ ] Test against real site (e.g., go.dev/doc/)\n- [ ] `make validate` passes\n- [ ] `CGO_ENABLED=0 go build` succeeds\n\n## References\n\n- docs/plans/2025-12-07-crawling-design.md","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-07T22:01:24.997083-08:00","updated_at":"2025-12-09T14:25:54.728434-08:00","closed_at":"2025-12-09T14:25:54.728436-08:00","dependencies":[{"issue_id":"locdoc-wfk","depends_on_id":"locdoc-mdd","type":"blocks","created_at":"2025-12-07T22:01:57.308742-08:00","created_by":"daemon"},{"issue_id":"locdoc-wfk","depends_on_id":"locdoc-mtz","type":"blocks","created_at":"2025-12-07T22:01:57.334324-08:00","created_by":"daemon"},{"issue_id":"locdoc-wfk","depends_on_id":"locdoc-a3y","type":"parent-child","created_at":"2025-12-07T22:02:05.057883-08:00","created_by":"daemon"}]}
{"id":"locdoc-x0w","title":"Centralize model name configuration in main","description":"## Problem\nModel name is currently defined in multiple places (gemini/asker.go, cmd/locdoc/main.go). Should be defined once in main and injected into components that need it.\n\n## Entrypoints\n- `cmd/locdoc/main.go` - define single model constant\n- `gemini/asker.go` - accept model as parameter to NewAsker\n- `gemini/token.go` - accept model as parameter to NewTokenCounter\n\n## Validation\n- [ ] Model name defined only in cmd/locdoc/main.go\n- [ ] Asker and TokenCounter accept model as constructor parameter\n- [ ] `make validate` passes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-17T12:14:31.7888-08:00","updated_at":"2025-12-17T20:44:02.924121-08:00","closed_at":"2025-12-17T20:44:02.924124-08:00"}
{"id":"locdoc-y27","title":"Switch to WaitStable and incognito contexts","description":"## Problem\nWaitLoad() waits for window.onload which can hang in background tabs. Raw page creation allows state contamination between concurrent requests.\n\n## Solution\n\n### 1. Replace WaitLoad with WaitStable\nWaitStable() combines multiple checks in parallel - if one event fails, others can complete.\n\n```go\n// Before\npage.WaitLoad()\n\n// After  \npage.WaitStable()\n```\n\n### 2. Use incognito contexts per request\n```go\nincognito := browser.MustIncognito()\npage := incognito.MustPage()\n// ... fetch ...\nincognito.MustClose()\n```\n\nIncognito adds negligible overhead (ms) vs new browser (2-3s).\n\n### 3. Fix page close with fresh context\nUse fresh context for cleanup since cancelled context will fail page.Close().\n\n## Entrypoints\n- rod/fetcher.go:Fetch()\n\n## Validation\n- [ ] WaitStable() used instead of WaitLoad()\n- [ ] Each fetch uses incognito context\n- [ ] Page cleanup uses fresh context\n- [ ] make validate passes\n\n## Research\nSee docs/go-rod-reliability.md","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T10:54:46.787353-08:00","updated_at":"2025-12-20T11:22:01.70925-08:00","closed_at":"2025-12-20T11:22:01.709253-08:00","dependencies":[{"issue_id":"locdoc-y27","depends_on_id":"locdoc-3ks","type":"blocks","created_at":"2025-12-20T10:54:51.259291-08:00","created_by":"daemon"}]}
{"id":"locdoc-ys8","title":"Implement link selector registry","description":"Create LinkSelectorRegistry that manages framework-specific selectors and auto-detects frameworks.\n\n## Entrypoints\n- Create goquery/registry.go - Registry struct with Get(), GetForHTML(), Register()\n- Create goquery/registry_test.go\n\n## Validation\n- Returns correct selector for each framework\n- Auto-detects framework from HTML\n- Falls back to generic selector for unknown frameworks\n- make validate passes","notes":"COMPLETED: LinkSelectorRegistry implementation with TDD\n\nCreated:\n- goquery/registry.go - Registry struct with Get(), GetForHTML(), Register(), List()\n- goquery/registry_test.go - 9 test cases covering all behaviors\n\nFeatures:\n- Returns correct selector for each framework via Get()\n- Auto-detects framework from HTML via GetForHTML()\n- Falls back to generic selector for unknown frameworks\n- Falls back when framework detected but no selector registered\n- Register() adds/overwrites selectors for frameworks\n- List() returns all registered frameworks\n\nAll tests pass, make validate passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-18T16:08:09.820916-08:00","updated_at":"2025-12-18T19:11:14.366435-08:00","closed_at":"2025-12-18T19:11:14.366438-08:00","dependencies":[{"issue_id":"locdoc-ys8","depends_on_id":"locdoc-2yj","type":"parent-child","created_at":"2025-12-18T16:16:39.800983-08:00","created_by":"daemon"},{"issue_id":"locdoc-ys8","depends_on_id":"locdoc-d6s","type":"blocks","created_at":"2025-12-18T16:18:25.005253-08:00","created_by":"daemon"},{"issue_id":"locdoc-ys8","depends_on_id":"locdoc-5lp","type":"blocks","created_at":"2025-12-18T16:18:25.069331-08:00","created_by":"daemon"}]}
{"id":"locdoc-ytd","title":"Simplify probeFetcher by requiring both fetchers","description":"## Problem\nprobeFetcher has defensive nil checks for HTTPFetcher, RodFetcher, Prober, and Extractor that add complexity. In practice, all four are always provided together.\n\n## Proposal\nRequire all four components when configuring probing:\n- HTTPFetcher, RodFetcher, Prober, and Extractor are all required\n- Remove all nil checks and fallback paths in probeFetcher\n- For DiscoverURLs: if probing is enabled, require all four options\n- Single validation point, then assume everything is present\n\n## Benefits\n- Simplest possible code path\n- Clear API contract - all or nothing\n- No edge cases or fallback logic\n- Reflects actual usage patterns\n\n## Entrypoints\n- crawl/crawl.go:probeFetcher\n- crawl/crawl.go:Crawler struct\n- crawl/discover.go:DiscoverURLs\n\n## Validation\n- [ ] probeFetcher has zero nil checks\n- [ ] All tests provide all four components\n- [ ] make validate passes","notes":"COMPLETED: Full implementation\n- DiscoverURLs signature changed with 4 required params\n- Removed With* probing options\n- Removed nil checks from probeFetcher\n- Updated Dependencies, add.go, main.go\n- Updated all tests\n- make validate passes\n- locdoc-6af merged into this work","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-20T18:00:27.737839-08:00","updated_at":"2025-12-20T18:56:39.316032-08:00","closed_at":"2025-12-20T18:56:39.316036-08:00"}
{"id":"locdoc-zml","title":"XML document structure with metadata","description":"## Problem\nChange document serialization from markdown headers to XML format with index, title, source URL, and content per document.\n\n## Entrypoints\n- gemini/asker.go: buildPrompt function, create new formatDocumentsXML function\n\n## Validation\n- [ ] Documents wrapped in \u003cdocuments\u003e with individual \u003cdocument\u003e elements\n- [ ] Each document has index, title, source, content fields\n- [ ] Title falls back to source URL if empty\n- [ ] make validate passes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-10T21:02:09.433362-08:00","updated_at":"2025-12-10T21:15:13.03919-08:00","closed_at":"2025-12-10T21:15:13.039192-08:00"}
{"id":"locdoc-zu7","title":"Review error messages for LLM-friendly guidance","description":"## Problem\n\nThis tool is designed for both humans and LLMs, with LLMs likely being the primary users. Error messages need to guide users toward correct usage, not just report failures.\n\n## Context\n\nGood CLI tools for LLM consumption follow a pattern: when something goes wrong, the error message explains:\n1. What failed\n2. Why it failed\n3. How to fix it\n\nExample of poor error: `invalid project`\nExample of good error: `project \"foo\" not found. Use \"locdoc list\" to see available projects.`\n\n## Entrypoints\n\n- `error.go` - existing error code infrastructure\n- `cmd/locdoc/*.go` - command implementations where errors surface\n\n## Work Required\n\n1. Audit all user-facing error paths\n2. Identify typical failure modes (wrong args, missing project, network issues, etc.)\n3. Ensure each error message includes actionable guidance\n4. Consider adding `--help` hints where appropriate\n\n## Validation\n\n- [ ] Each error path reviewed\n- [ ] Common failure modes have helpful messages\n- [ ] Messages tested with LLM to verify they guide toward correct usage\n- [ ] `make validate` passes","notes":"COMPLETED:\n- Audited all user-facing error paths\n- Improved error messages in add.go (regex filter), main.go (usage, database, browser, API key)\n- Added tests for regex filter, usage, and database errors\n\nIMPROVED MESSAGES:\n1. Invalid regex filter: now shows example patterns\n2. No command: now says 'locdoc --help' for help\n3. Database open: shows path and mentions LOCDOC_DB env var\n4. Browser start: mentions Chrome/Chromium requirement\n5. API key: improved to mention URL for getting key\n\nALREADY GOOD (unchanged):\n- Project not found errors (mention 'locdoc list')\n- No documents error (shows re-add instructions)\n- Delete without --force (clear guidance)\n- No projects message (mentions 'locdoc add')","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-09T15:59:26.347465-08:00","updated_at":"2025-12-12T07:52:54.793632-08:00","closed_at":"2025-12-12T07:52:54.793634-08:00"}
